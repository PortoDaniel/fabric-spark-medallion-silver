{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e458681-90f6-451f-a9cc-c28371fff3f7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first, col, max as spark_max, lit, trim, current_timestamp, regexp_replace, when, row_number, coalesce, to_timestamp, lower, size, split, element_at\n",
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql.types import DoubleType, TimestampType, IntegerType\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F, types as T , Window\n",
    "from collections import Counter, defaultdict\n",
    "import unicodedata\n",
    "import re, unicodedata\n",
    "import time\n",
    "from pyspark import StorageLevel\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a21ecb-acb8-45bb-b00b-97839a4a4d53",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Spark otimizado para DataFrames at√© 300 linhas\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 1)   # Menos shuffle\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")  # Mais r√°pido Pandas ‚áÜ Spark\n",
    "spark.conf.set(\"spark.databricks.delta.merge.enableLowShuffle\", \"true\")  # Merge r√°pido em tabelas pequenas\n",
    "\n",
    "# Compacta√ß√£o autom√°tica desligada (n√£o faz sentido para tabelas pequenas)\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"false\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 1)\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055e808-c867-48fc-afdb-c43d92a66223",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "spark.conf.get(\"spark.sql.session.timeZone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29205860-be07-440f-b4d5-43e48dceb98d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ 2) Caminhos dos Lakehouses ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Lakehouse Bronze (onde est√£o as tabelas originais)\n",
    "bronze_lakehouse_path = (\n",
    "    \"abfss://<GUID>@\"\n",
    "    \"onelake.dfs.fabric.microsoft.com/\"\n",
    "    \"<GUID>/Tables\"\n",
    ")\n",
    "\n",
    "# Lakehouse Prata (onde voc√™ est√° rodando este notebook)\n",
    "prata_lakehouse_path = (\n",
    "    \"abfss://<CONTAINER>@onelake.dfs.fabric.microsoft.com/<WORKSPACE_ID>/<PATH>\"\n",
    ")\n",
    "\n",
    "print(\"üìÇ Bronze:\", bronze_lakehouse_path)\n",
    "print(\"üìÇ Prata :\", prata_lakehouse_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd199eaa-0332-44fd-abb5-95ba43512eb2",
   "metadata": {
    "editable": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# üßπ EXCLUS√ÉO DE REGISTROS INSERIDOS NESTE M√äS (FUSO BRT)\n",
    "# ===========================================================\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# Timezone de Bras√≠lia\n",
    "BRT = ZoneInfo(\"America/Sao_Paulo\")\n",
    "\n",
    "# Lista de tabelas Bronze a limpar\n",
    "tabelas_para_limpar = [\n",
    "    \"venda_prata\",\n",
    "    \"unidade_prata\",\n",
    "    \"reservas_workflow_prata\",\n",
    "    \"reservas_registros_flags_prata\",\n",
    "    \"reserva_prata\",\n",
    "    \"repasse_prata\",\n",
    "    \"precadastros_workflow_prata\",\n",
    "    \"precadastro_prata\",\n",
    "    \"leads_workflow_prata\",\n",
    "    \"lead_prata\",\n",
    "    \"imobiliarias_prata\",\n",
    "    \"corretores_prata\",\n",
    "    \"campos_adicionais_prata\"\n",
    "]\n",
    "\n",
    "# Primeiro dia do m√™s atual (ajustado para Bras√≠lia)\n",
    "data_corte = datetime(2025, 10, 31, 0, 0, 0, tzinfo=BRT)\n",
    "print(f\"üïí Considerando registros com ingest_ts >= {data_corte} (fuso BRT)\\n\")\n",
    "\n",
    "for tabela in tabelas_para_limpar:\n",
    "    print(f\"üßπ Limpando registros recentes da tabela: {tabela}\")\n",
    "\n",
    "    try:\n",
    "        if not DeltaTable.isDeltaTable(spark, f\"Tables/{tabela}\"):\n",
    "            print(f\"‚ö†Ô∏è  Tabela {tabela} n√£o √© Delta ou n√£o existe ‚Äî pulando.\\n\")\n",
    "            continue\n",
    "\n",
    "        delta_tbl = DeltaTable.forName(spark, tabela)\n",
    "        # Condi√ß√£o: registros inseridos a partir do primeiro dia do m√™s\n",
    "        condicao = F.col(\"ingest_ts\") >= F.lit(data_corte)\n",
    "        # Executa o delete\n",
    "        delta_tbl.delete(condicao)\n",
    "        print(f\"‚úÖ Registros deste m√™s removidos com sucesso de {tabela}.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao limpar {tabela}: {e}\\n\")\n",
    "\n",
    "print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(\"üß© Limpeza conclu√≠da em todas as tabelas listadas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b947aff-0b8c-4df0-8fe4-967f57b1b2ed",
   "metadata": {
    "editable": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# üßπ DROP DAS TABELAS PRATA (APENAS NO PRIMEIRO DIA DO M√äS)\n",
    "# ===========================================================\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# Timezone BRT\n",
    "BRT = ZoneInfo(\"America/Sao_Paulo\")\n",
    "agora = datetime.now(BRT)\n",
    "\n",
    "# Identificar primeiro dia do m√™s\n",
    "primeiro_dia_mes = datetime(agora.year, agora.month, 1, tzinfo=BRT)\n",
    "\n",
    "# ===========================================================\n",
    "# üîç VERIFICA√á√ÉO: s√≥ roda o DROP se for o primeiro dia\n",
    "# ===========================================================\n",
    "\n",
    "if agora.date() == primeiro_dia_mes.date():\n",
    "\n",
    "    print(\"‚úÖ Hoje √© o PRIMEIRO dia do m√™s. Executando DROP das tabelas PRATA...\\n\")\n",
    "\n",
    "    tabelas_para_drop = [\n",
    "        \"venda_prata\",\n",
    "        \"unidade_prata\",\n",
    "        \"reservas_workflow_prata\",\n",
    "        \"reservas_registros_flags_prata\",\n",
    "        \"reserva_prata\",\n",
    "        \"repasse_prata\",\n",
    "        \"precadastros_workflow_prata\",\n",
    "        \"precadastro_prata\",\n",
    "        \"leads_workflow_prata\",\n",
    "        \"lead_prata\",\n",
    "        \"imobiliarias_prata\",\n",
    "        \"corretores_prata\",\n",
    "        \"campos_adicionais_prata\"\n",
    "    ]\n",
    "\n",
    "    # Executa o DROP TABLE para cada tabela\n",
    "    for tabela in tabelas_para_drop:\n",
    "        print(f\"üßπ Dropando tabela: {tabela}\")\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {tabela}\")\n",
    "            print(f\"üóëÔ∏è Tabela {tabela} removida com sucesso.\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao dropar a tabela {tabela}: {e}\\n\")\n",
    "\n",
    "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "    print(\"üß© DROP TOTAL (PRATA) finalizado com sucesso.\")\n",
    "\n",
    "else:\n",
    "    print(\"üîï N√£o √© o PRIMEIRO dia do m√™s. Pulando o DROP das tabelas PRATA.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2573932-3e40-40c9-8a93-878edb0d06b2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def ler_incremental_bronze(\n",
    "    nome_tabela: str,\n",
    "    path_bronze: str,\n",
    "    path_prata: str,\n",
    "    id_col: str,\n",
    "    ids_alternativos: list = None\n",
    "):\n",
    "    \"\"\"\n",
    "    L√™ o incremental da Bronze SEM CDF.\n",
    "    Sempre usa referencia_data da Bronze como base de incremental.\n",
    "    Como as tabelas PRATA s√£o dropadas no 1¬∫ dia do m√™s, PRATA inexistente = full load.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nüîç Lendo incremental da tabela '{nome_tabela}'...\")\n",
    "\n",
    "    if ids_alternativos is None:\n",
    "        ids_alternativos = [id_col]\n",
    "\n",
    "    # 1Ô∏è‚É£ Se a PRATA n√£o existe ‚Äî FULL LOAD\n",
    "    if not DeltaTable.isDeltaTable(spark, path_prata):\n",
    "        print(f\"‚ö†Ô∏è PRATA '{nome_tabela}' inexistente ‚Äî lendo Bronze completa (primeira carga).\")\n",
    "        df_bronze_full = spark.read.format(\"delta\").load(path_bronze)\n",
    "        print(f\"‚úÖ {nome_tabela}: Bronze carregada completamente ‚Äî {df_bronze_full.count()} registros.\")\n",
    "        return df_bronze_full\n",
    "\n",
    "    # 2Ô∏è‚É£ PRATA existe ‚Üí fazer incremental baseado no MAX(referencia_data)\n",
    "    df_bronze_base = spark.read.format(\"delta\").load(path_bronze)\n",
    "    df_prata_base  = spark.read.format(\"delta\").load(path_prata)\n",
    "\n",
    "    # Verifica se existe referencia_data nas duas\n",
    "    bronze_cols = [c.lower() for c in df_bronze_base.columns]\n",
    "    prata_cols  = [c.lower() for c in df_prata_base.columns]\n",
    "\n",
    "    if \"referencia_data\" not in bronze_cols:\n",
    "        print(\"‚ö†Ô∏è Bronze n√£o possui referencia_data ‚Üí FULL REFRESH da Bronze.\")\n",
    "        return df_bronze_base\n",
    "\n",
    "    if \"referencia_data\" not in prata_cols:\n",
    "        print(\"‚ö†Ô∏è Prata n√£o possui referencia_data ‚Üí FULL REFRESH da Bronze.\")\n",
    "        return df_bronze_base\n",
    "\n",
    "    # 3Ô∏è‚É£ Descobre a maior referencia_data da PRATA\n",
    "    ultima_ref = (\n",
    "        df_prata_base\n",
    "        .agg(F.max(\"referencia_data\").alias(\"max_ref\"))\n",
    "        .collect()[0][\"max_ref\"]\n",
    "    )\n",
    "\n",
    "    if not ultima_ref:\n",
    "        print(\"‚ö†Ô∏è PRATA sem referencia_data v√°lida ‚Üí FULL REFRESH.\")\n",
    "        return df_bronze_base\n",
    "\n",
    "    print(f\"üìÖ √öltima referencia_data processada na PRATA: {ultima_ref}\")\n",
    "\n",
    "    # 4Ô∏è‚É£ Filtrar Bronze NOVA (incremental)\n",
    "    df_incremental_bronze = df_bronze_base.filter(\n",
    "        F.col(\"referencia_data\") > F.lit(ultima_ref)\n",
    "    )\n",
    "\n",
    "    print(f\"üìà Registros incrementais encontrados: {df_incremental_bronze.count()}\")\n",
    "\n",
    "    # üîÑ Anti-join para evitar duplicados id + referencia_data\n",
    "    df_prata_ref = df_prata_base.select(id_col, \"referencia_data\")\n",
    "\n",
    "    df_incremental_final = (\n",
    "        df_incremental_bronze.alias(\"b\")\n",
    "        .join(\n",
    "            df_prata_ref.alias(\"p\"),\n",
    "            (F.col(f\"b.{id_col}\") == F.col(f\"p.{id_col}\")) &\n",
    "            (F.col(\"b.referencia_data\") == F.col(\"p.referencia_data\")),\n",
    "            \"leftanti\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Incremental pronto ‚Äî {df_incremental_final.count()} registros √∫nicos.\")\n",
    "    return df_incremental_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2d0bc9-36fe-4f63-8369-ab783ea974ad",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## [**Fatos**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20f5b4-7b44-4be9-8062-bec19e057074",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ‚öôÔ∏è CAMINHOS F√çSICOS DAS TABELAS BRONZE\n",
    "# ================================================================\n",
    "path_leads         = f\"{bronze_lakehouse_path}/leads_bronze_cvdw\"\n",
    "path_repasse       = f\"{bronze_lakehouse_path}/repasses_bronze_cvdw\"\n",
    "path_reserva       = f\"{bronze_lakehouse_path}/reservas_bronze_cvdw\"\n",
    "path_precadastro   = f\"{bronze_lakehouse_path}/precadastros_bronze_cvdw\"\n",
    "\n",
    "# ================================================================\n",
    "# ‚öôÔ∏è CAMINHOS F√çSICOS DAS TABELAS PRATA\n",
    "# ================================================================\n",
    "path_leads_prata        = f\"{prata_lakehouse_path}/lead_prata\"\n",
    "path_repasse_prata      = f\"{prata_lakehouse_path}/repasse_prata\"\n",
    "path_reserva_prata      = f\"{prata_lakehouse_path}/reserva_prata\"\n",
    "path_precadastro_prata  = f\"{prata_lakehouse_path}/precadastro_prata\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b42f59f-259b-4318-b4a1-d695c4205230",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- LEADS -------------------\n",
    "df_leads_bronze = ler_incremental_bronze(\n",
    "    nome_tabela=\"leads\",\n",
    "    path_bronze=path_leads,\n",
    "    path_prata=path_leads_prata,\n",
    "    id_col=\"idlead\"\n",
    ")\n",
    "if df_leads_bronze is not None and not df_leads_bronze.rdd.isEmpty():\n",
    "    df_leads_bronze = df_leads_bronze.repartition(2, F.col(\"idlead\"))  # üëà define 4 parti√ß√µes fixas\n",
    "    print(f\"‚úÖ df_leads_bronze criado com sucesso ‚Äî {df_leads_bronze.count()} registros incrementais.\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado incremental carregado para 'leads'.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd3110-e138-4d1c-84f4-e0880929c222",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- PRECADASTRO -------------------\n",
    "df_precadastro_bronze = ler_incremental_bronze(\n",
    "    nome_tabela=\"precadastro\",\n",
    "    path_bronze=path_precadastro,\n",
    "    path_prata=path_precadastro_prata,\n",
    "    id_col=\"idprecadastro\"\n",
    ")\n",
    "if df_precadastro_bronze is not None and not df_precadastro_bronze.rdd.isEmpty():\n",
    "    print(f\"‚úÖ df_precadastro_bronze criado com sucesso ‚Äî {df_precadastro_bronze.count()} registros incrementais.\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado incremental carregado para 'precadastro'.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec01de40-9a1f-49cb-9588-2283845dcc44",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- RESERVA -------------------\n",
    "df_reserva_bronze = ler_incremental_bronze(\n",
    "    nome_tabela=\"reserva\",\n",
    "    path_bronze=path_reserva,\n",
    "    path_prata=path_reserva_prata,\n",
    "    id_col=\"idreserva\"\n",
    ")\n",
    "if df_reserva_bronze is not None and not df_reserva_bronze.rdd.isEmpty():\n",
    "    print(f\"‚úÖ df_reserva_bronze criado com sucesso ‚Äî {df_reserva_bronze.count()} registros incrementais.\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado incremental carregado para 'reserva'.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a89f5f-525f-4216-9736-2eb971ad04e3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- REPASSE -------------------\n",
    "df_repasse_bronze = ler_incremental_bronze(\n",
    "    nome_tabela=\"repasse\",\n",
    "    path_bronze=path_repasse,\n",
    "    path_prata=path_repasse_prata,\n",
    "    id_col=\"idrepasse\"\n",
    ")\n",
    "if df_repasse_bronze is not None and not df_repasse_bronze.rdd.isEmpty():\n",
    "    print(f\"‚úÖ df_repasse_bronze criado com sucesso ‚Äî {df_repasse_bronze.count()} registros incrementais.\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado incremental carregado para 'repasse'.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d517d1-97a2-42dd-a727-a1899813a6fa",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def normalizar_nomes_colunas(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Normaliza os nomes das colunas:# Registrar as views no cat√°logo tempor√°rio\n",
    "df_leads_bronze.createOrReplaceTempView(\"leads_bronze\")\n",
    "df_repasse_bronze.createOrReplaceTempView(\"repasse_bronze\")\n",
    "df_reserva_bronze.createOrReplaceTempView(\"reserva_bronze\")\n",
    "df_precadastro_bronze.createOrReplaceTempView(\"precadastro_bronze\")\n",
    "      - Remove acentos e pontua√ß√µes\n",
    "      - Substitui espa√ßos e h√≠fens por \"_\"\n",
    "      - Remove ., ,, ?, !, :, ;, etc.\n",
    "      - Converte tudo para min√∫sculo\n",
    "    Retorna um novo DataFrame com colunas renomeadas\n",
    "    \"\"\"\n",
    "\n",
    "    def limpar_nome(col_name: str) -> str:\n",
    "        # remove acentos\n",
    "        sem_acentos = unicodedata.normalize(\"NFKD\", col_name).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n",
    "        # remove pontua√ß√µes e substitui por nada\n",
    "        sem_pontuacao = re.sub(r\"[.,;:!?\\\"'()%$#@/\\\\-]\", \"\", sem_acentos)\n",
    "        # troca espa√ßos e m√∫ltiplos underscores por _\n",
    "        com_underscore = re.sub(r\"\\s+\", \"_\", sem_pontuacao)\n",
    "        # remove underscores duplos ou no in√≠cio/fim\n",
    "        limpo = re.sub(r\"_+\", \"_\", com_underscore).strip(\"_\")\n",
    "        # tudo min√∫sculo\n",
    "        return limpo.lower()\n",
    "\n",
    "    # Aplica renomea√ß√£o de forma iterativa\n",
    "    for old_name in df.columns:\n",
    "        new_name = limpar_nome(old_name)\n",
    "        if new_name != old_name:\n",
    "            df = df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "    print(f\"‚úÖ Colunas normalizadas ({len(df.columns)} colunas)\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981bcac2-e182-42c1-9ee1-e7e919e4494e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df_precadastro_bronze = normalizar_nomes_colunas(df_precadastro_bronze)\n",
    "df_reserva_bronze     = normalizar_nomes_colunas(df_reserva_bronze)\n",
    "df_repasse_bronze     = normalizar_nomes_colunas(df_repasse_bronze)\n",
    "df_leads_bronze       = normalizar_nomes_colunas(df_leads_bronze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1e3499-729a-4ad3-b593-b276077a3e0f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### [**Tratar Valores e Tipagem**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e04c7-cbec-4132-9c15-cc0635895c7a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# üîπ Pr√©-compila√ß√£o de regex ‚Äî evita recriar padr√µes a cada chamada\n",
    "_RE_NON_ALNUM = re.compile(r\"[^A-Za-z0-9_]+\")\n",
    "_RE_MULTI_UNDERSCORE = re.compile(r\"_+\")\n",
    "\n",
    "def _norm_basic(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza nomes de colunas para compara√ß√£o robusta (sem acentos, espa√ßos, s√≠mbolos).\n",
    "    Exemplo: \"Data de Nascimento\" -> \"data_de_nascimento\"\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s2 = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    s2 = _RE_NON_ALNUM.sub(\"_\", s2.strip().lower())\n",
    "    s2 = _RE_MULTI_UNDERSCORE.sub(\"_\", s2).strip(\"_\")\n",
    "    return s2\n",
    "\n",
    "\n",
    "def _map_list_by_normalization(df_cols, lista):\n",
    "    \"\"\"\n",
    "    Mapeia nomes normalizados de 'lista' para nomes reais do DataFrame.\n",
    "    Exemplo: ['data_cadastro'] pode casar com ['Data Cadastro', 'data_cad'] etc.\n",
    "    \"\"\"\n",
    "    if not lista:\n",
    "        return []\n",
    "\n",
    "    # üîπ Monta dicion√°rio normalizado ‚Üí nomes reais\n",
    "    norm2real = {}\n",
    "    for c in df_cols:\n",
    "        norm = _norm_basic(c)\n",
    "        norm2real.setdefault(norm, []).append(c)\n",
    "\n",
    "    resolved = []\n",
    "    seen = set()\n",
    "\n",
    "    for item in lista:\n",
    "        key = _norm_basic(item)\n",
    "        hits = norm2real.get(key)\n",
    "        if not hits:\n",
    "            continue\n",
    "        for h in hits:\n",
    "            if h not in seen:\n",
    "                resolved.append(h)\n",
    "                seen.add(h)\n",
    "\n",
    "    return resolved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032188bd-62df-4855-a253-5f6149a4d3cc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# 6B) TRATAR_VALORES_E_DATAS (vers√£o avan√ßada Hudson)\n",
    "# =====================================================================\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "def tratar_valores_e_datas(\n",
    "    df,\n",
    "    colunas_valor: list = None,\n",
    "    colunas_data: list = None,\n",
    "    excluir_valor: list = None,\n",
    "    excluir_data: list = None,\n",
    "    forcar_valor: list = None,\n",
    "    forcar_data: list = None,\n",
    "    zero_um_como_nulo: bool = False,\n",
    "    explicit_prefix: str = None,\n",
    "    limitar_ao_prefixo: bool = None,\n",
    "    auto_prefix_listas: bool = None,\n",
    "    match_normalizado_nas_listas: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Converte colunas de valor -> Double e colunas de data -> Timestamp,\n",
    "    limpando apenas as c√©lulas inv√°lidas de cada coluna (nunca a linha toda).\n",
    "    Corrige '... .0' em datas e filtra anos fora de [1900,2100].\n",
    "    \"\"\"\n",
    "    df_tratado = df\n",
    "    dtypes = dict(df.dtypes)\n",
    "    df_cols = df.columns\n",
    "\n",
    "    # Defaults\n",
    "    if limitar_ao_prefixo is None:\n",
    "        limitar_ao_prefixo = bool(explicit_prefix)\n",
    "    if auto_prefix_listas is None:\n",
    "        auto_prefix_listas = bool(explicit_prefix)\n",
    "    prefix = explicit_prefix or \"\"\n",
    "\n",
    "    # Helpers\n",
    "    def base_name(c):\n",
    "        return c[len(prefix):] if (prefix and c.startswith(prefix)) else c\n",
    "\n",
    "    # Inicializa listas\n",
    "    colunas_valor = list(colunas_valor or [])\n",
    "    colunas_data  = list(colunas_data  or [])\n",
    "    excluir_valor = list(excluir_valor or [])\n",
    "    excluir_data  = list(excluir_data or [])\n",
    "    forcar_valor  = list(forcar_valor or [])\n",
    "    forcar_data   = list(forcar_data or [])\n",
    "\n",
    "    # Candidatas por prefixo\n",
    "    candidatas = [c for c in df_cols if (not limitar_ao_prefixo or c.startswith(prefix))]\n",
    "\n",
    "    # Heur√≠sticas\n",
    "    palavras_valor = [\"valor\", \"renda\", \"preco\", \"pre√ßo\", \"subsidio\", \"financi\", \"fgts\", \"itbi\", \"laudo\"]\n",
    "    palavras_data  = [\"data\", \"nascimento\", \"envio\", \"assinatura\", \"vencimento\", \"cadastro\", \"alteracao\", \"atualizacao\"]\n",
    "\n",
    "    auto_valor = [c for c in candidatas if any(p in base_name(c).lower() for p in palavras_valor)]\n",
    "    auto_data  = [c for c in candidatas if any(p in base_name(c).lower() for p in palavras_data)]\n",
    "\n",
    "    # Montagem final (respeita excluir/forcar)\n",
    "    colunas_valor = sorted(set(colunas_valor + auto_valor + forcar_valor) - set(excluir_valor))\n",
    "    colunas_data  = sorted(set(colunas_data  + auto_data  + forcar_data)  - set(excluir_data))\n",
    "\n",
    "    # Logs √∫teis\n",
    "    print(\"üéØ Configura√ß√£o aplicada:\")\n",
    "    print(f\"   üí∞ For√ßar valor: {', '.join(forcar_valor) if forcar_valor else 'Nenhuma'}\")\n",
    "    print(f\"   üìÖ For√ßar data:  {', '.join(forcar_data)  if forcar_data  else 'Nenhuma'}\")\n",
    "    print(f\"   üö´ Excluir valor: {', '.join(excluir_valor) if excluir_valor else 'Nenhuma'}\")\n",
    "    print(f\"   üö´ Excluir data:  {', '.join(excluir_data)  if excluir_data  else 'Nenhuma'}\")\n",
    "    print(f\"   üí∞ Colunas finais de valor ({len(colunas_valor)}): {colunas_valor}\")\n",
    "    print(f\"   üìÖ Colunas finais de data  ({len(colunas_data)}): {colunas_data}\")\n",
    "\n",
    "    # 1Ô∏è‚É£ IDs -> Integer (quando forem strings num√©ricas)\n",
    "    for c in [x for x in df_cols if x.lower().startswith(\"id\")]:\n",
    "        if dtypes.get(c) == \"string\":\n",
    "            df_tratado = df_tratado.withColumn(\n",
    "                c,\n",
    "                F.when(F.trim(F.col(c)).rlike(r\"^[0-9]+$\"), F.col(c).cast(T.IntegerType())).otherwise(F.col(c))\n",
    "            )\n",
    "\n",
    "    # 2Ô∏è‚É£ Valores -> Double\n",
    "    for c in colunas_valor:\n",
    "        if c in df_cols:\n",
    "            s = F.col(c).cast(\"string\")\n",
    "            s = F.regexp_replace(s, r\"(?i)\\s*(r\\$|rs|\\$|‚Ç¨|¬£|:)\", \"\")\n",
    "            s = F.regexp_replace(s, r\"\\.\", \"\")\n",
    "            s = F.regexp_replace(s, r\",\", \".\")\n",
    "            s = F.regexp_replace(s, r\"[^0-9\\.\\-\\+]\", \"\")\n",
    "            s = F.trim(s)\n",
    "            n = F.when(s.rlike(r\"^[+-]?\\d+(\\.\\d+)?$\"), s.cast(T.DoubleType())).otherwise(None)\n",
    "            if zero_um_como_nulo:\n",
    "                n = F.when((n <= 1.0) | n.isNull(), None).otherwise(n)\n",
    "            df_tratado = df_tratado.withColumn(c, n)\n",
    "\n",
    "    # 3Ô∏è‚É£ Datas -> Timestamp\n",
    "    def _parse_ts_string(scol):\n",
    "        s = F.trim(scol.cast(\"string\"))\n",
    "        s = F.regexp_replace(s, r\"[Tt]\", \" \")\n",
    "        s = F.regexp_replace(s, r\"\\s*Z$\", \"\")\n",
    "        s = F.regexp_replace(s, r\"(\\d{2}:\\d{2}:\\d{2})\\.\\d+\\s*$\", r\"\\1\")\n",
    "        s = F.regexp_replace(s, r\"/\", \"-\")\n",
    "\n",
    "        ts = F.coalesce(\n",
    "            F.col(c).cast(\"timestamp\"),\n",
    "            F.to_timestamp(s, \"yyyy-MM-dd HH:mm:ss\"),\n",
    "            F.to_timestamp(s, \"yyyy-MM-dd\"),\n",
    "            F.to_timestamp(s, \"dd-MM-yyyy HH:mm:ss\"),\n",
    "            F.to_timestamp(s, \"dd-MM-yyyy\"),\n",
    "            F.to_timestamp(s, \"MM-dd-yyyy HH:mm:ss\"),\n",
    "            F.to_timestamp(s, \"MM-dd-yyyy\")\n",
    "        )\n",
    "        ts = F.when((s.isNull()) | (s == \"\") | (F.lower(s) == \"null\"), None).otherwise(ts)\n",
    "        ts = F.when(ts.isNull() | (F.year(ts) < 1900) | (F.year(ts) > 2100), None).otherwise(ts)\n",
    "        return ts\n",
    "\n",
    "    for c in colunas_data:\n",
    "        if c in df_cols:\n",
    "            df_tratado = df_tratado.withColumn(c, _parse_ts_string(F.col(c)))\n",
    "\n",
    "    print(\"‚úÖ Tratamento conclu√≠do ‚Äî convers√µes aplicadas sem afetar colunas saud√°veis da mesma linha.\")\n",
    "    return df_tratado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e341a0-162b-4836-9122-da8c1a1f8933",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "def sanitize_for_sql_endpoint(df):\n",
    "    \"\"\"\n",
    "    Garante compatibilidade com o SQL Endpoint do Fabric:\n",
    "    - Em colunas string: \"\", \" \", \"null\", \"nan\", \"inf\", \"infinity\" (qualquer caixa) -> NULL\n",
    "    - Em colunas num√©ricas (Float/Double): NaN, +Inf, -Inf -> NULL\n",
    "    N√£o altera tipos das colunas.\n",
    "    \"\"\"\n",
    "    out = df\n",
    "\n",
    "    # Strings \"vazias\" ou marcadores viram NULL\n",
    "    STR_NULLS = [\"\", \" \", \"null\", \"NULL\", \"nan\", \"NaN\", \"inf\", \"Inf\", \"infinity\", \"Infinity\"]\n",
    "    for f in out.schema.fields:\n",
    "        c = f.name\n",
    "        if isinstance(f.dataType, T.StringType):\n",
    "            out = out.withColumn(\n",
    "                c,\n",
    "                F.when(F.trim(F.col(c)).isin(STR_NULLS), None).otherwise(F.col(c))\n",
    "            )\n",
    "\n",
    "    # Floats/Doubles: NaN e ¬±Infinity viram NULL\n",
    "    for f in out.schema.fields:\n",
    "        c = f.name\n",
    "        if isinstance(f.dataType, (T.FloatType, T.DoubleType)):\n",
    "            out = out.withColumn(\n",
    "                c,\n",
    "                F.when(\n",
    "                    F.isnan(F.col(c)) | (F.col(c) == float(\"inf\")) | (F.col(c) == float(\"-inf\")),\n",
    "                    None\n",
    "                ).otherwise(F.col(c))\n",
    "            )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28886d01-d6d2-4b6e-afdc-32870a5530fb",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# exclus√µes usando nomes *normalizados* (a fun√ß√£o resolve para o nome real)\n",
    "excluir_valor = [\"aprovacao_vpl_valor\"]\n",
    "excluir_data  = [\"status_registro\", \"juros_apos_entrega_cadastro\", \"idprecadastro\", \"juros_cadastro\", \"juros_cadastro_fixa_adicional\", \"vencimento\"]\n",
    "\n",
    "df_reserva_bronze = tratar_valores_e_datas(\n",
    "    df_reserva_bronze,\n",
    "    excluir_valor=excluir_valor,\n",
    "    excluir_data=excluir_data,\n",
    "    explicit_prefix=None,          # sem prefixo\n",
    "    limitar_ao_prefixo=False,      # varre todas as colunas\n",
    "    auto_prefix_listas=False,      # n√£o tenta prefixar listas\n",
    "    match_normalizado_nas_listas=True  # casa nomes normalizados com os reais\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RESERVA tratados e tipados com sucesso.\")\n",
    "df_reserva_bronze.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa5621-2d78-47f5-be22-6c04f2a7b68c",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "excluir_valor = [\"possibilidade_venda\", \"inserido_bolsao\", \"score\"]\n",
    "excluir_data  = []\n",
    "\n",
    "df_leads_bronze = tratar_valores_e_datas(\n",
    "    df_leads_bronze,\n",
    "    excluir_valor=excluir_valor,\n",
    "    excluir_data=excluir_data,\n",
    "    explicit_prefix=None,\n",
    "    limitar_ao_prefixo=False,\n",
    "    auto_prefix_listas=False,\n",
    "    match_normalizado_nas_listas=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LEAD tratados e tipados com sucesso.\")\n",
    "df_leads_bronze.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a7af1c-8aa4-4561-b4a0-78fdb03cefd6",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "excluir_valor = [\"itbi_pago\", \"recebendo_financiamento\", \"data_laudo_liberado\", \"data_status_financiamento\"]\n",
    "excluir_data  = [\"liberar_assinatura\", \"registro_pago\", \"valor_registro\"]\n",
    "colunas_data  = [\"ultima_atualizacao\", \"data_laudo_liberado\", \"data_status_financiamento\"]  # for√ßa tratamento dessa coluna como data\n",
    "\n",
    "df_repasse_bronze = tratar_valores_e_datas(\n",
    "    df_repasse_bronze,\n",
    "    excluir_valor=excluir_valor,\n",
    "    excluir_data=excluir_data,\n",
    "    colunas_data=colunas_data,\n",
    "    explicit_prefix=None,\n",
    "    limitar_ao_prefixo=False,\n",
    "    auto_prefix_listas=False,\n",
    "    match_normalizado_nas_listas=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ REPASSE tratados e tipados com sucesso.\")\n",
    "df_repasse_bronze.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2239ceb-28f6-4523-98a5-0c1d49ca8e08",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "excluir_valor = [\"carta_credito\"]\n",
    "excluir_data  = [\"vencimento\", \"idprecadastro\", \"sla_vencimento\"]\n",
    "\n",
    "df_precadastro_bronze = tratar_valores_e_datas(\n",
    "    df_precadastro_bronze,\n",
    "    excluir_valor=excluir_valor,\n",
    "    excluir_data=excluir_data,\n",
    "    explicit_prefix=None,\n",
    "    limitar_ao_prefixo=False,\n",
    "    auto_prefix_listas=False,\n",
    "    match_normalizado_nas_listas=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ PRECADASTRO tratados e tipados com sucesso.\")\n",
    "df_precadastro_bronze.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5379d0-68b4-4160-b35c-bd238c8add83",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def preparar_para_prata(\n",
    "    df,\n",
    "    coluna_id: str,\n",
    "    sistema_origem: str = \"CVCRM\",\n",
    "    filtrar_ativo: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o otimizada para uso em Lakehouse do Microsoft Fabric:\n",
    "    üîπ Deduplica√ß√£o eficiente por ID e data\n",
    "    üîπ Convers√£o robusta de IDs m√∫ltiplos e strings vazias\n",
    "    üîπ Redu√ß√£o de a√ß√µes (menos collect/count)\n",
    "    üîπ Ideal para uso em cluster com paralelismo controlado\n",
    "    \"\"\"\n",
    "\n",
    "    # üîí Valida√ß√£o\n",
    "    if \"referencia_data\" not in df.columns:\n",
    "        raise ValueError(\"A coluna obrigat√≥ria 'referencia_data' n√£o existe no DataFrame.\")\n",
    "\n",
    "    # 1Ô∏è‚É£ IDs v√°lidos\n",
    "    base = df.filter((F.col(coluna_id).isNotNull()) & (trim(F.col(coluna_id)) != \"\"))\n",
    "\n",
    "    # 2Ô∏è‚É£ Filtro ativo (opcional)\n",
    "    if filtrar_ativo and \"ativo\" in base.columns:\n",
    "        base = base.filter((F.col(\"ativo\").isNull()) | (F.col(\"ativo\") == F.lit(\"S\")))\n",
    "\n",
    "    # 3Ô∏è‚É£ Deduplica√ß√£o por ID ‚Äî mais recente por referencia_data\n",
    "    w = Window.partitionBy(coluna_id).orderBy(F.col(\"referencia_data\").desc(), F.monotonically_increasing_id())\n",
    "    base = (\n",
    "        base.withColumn(\"rn\", row_number().over(w))\n",
    "            .filter(F.col(\"rn\") == 1)\n",
    "            .drop(\"rn\")\n",
    "    )\n",
    "\n",
    "    # 4Ô∏è‚É£ Strings vazias ‚Üí NULL (em lote, 1 select)\n",
    "    exprs = [\n",
    "        when(trim(F.col(c)) == \"\", F.lit(None)).otherwise(F.col(c)).alias(c)\n",
    "        if t == \"string\" else F.col(c)\n",
    "        for c, t in base.dtypes\n",
    "    ]\n",
    "    base = base.select(*exprs)\n",
    "\n",
    "    # 5Ô∏è‚É£ Detectar colunas com m√∫ltiplos IDs (com v√≠rgulas)\n",
    "    colunas_virgula = [\n",
    "        c for c, t in base.dtypes\n",
    "        if t == \"string\" and c.lower().startswith(\"id\") and base.filter(F.col(c).contains(\",\")).limit(1).count() > 0\n",
    "    ]\n",
    "\n",
    "    for c in colunas_virgula:\n",
    "        print(f\"‚öôÔ∏è Coluna '{c}' cont√©m v√≠rgulas ‚Äî expandindo automaticamente as posi√ß√µes.\")\n",
    "\n",
    "        # Split + m√°ximo tamanho (1 a√ß√£o collect)\n",
    "        base = base.withColumn(\"_split\", split(trim(F.col(c)), \",\"))\n",
    "        max_pos = base.select(F.max(size(F.col(\"_split\")))).first()[0] or 0\n",
    "\n",
    "        # Cria√ß√£o das novas colunas dinamicamente\n",
    "        for i in range(max_pos):\n",
    "            pos_col = f\"posicao_{i+1}_{c}\"\n",
    "            base = base.withColumn(\n",
    "                pos_col,\n",
    "                F.when(F.col(\"_split\").getItem(i).rlike(\"^[0-9]+$\"),\n",
    "                       F.col(\"_split\").getItem(i).cast(\"int\")).otherwise(F.lit(None))\n",
    "            )\n",
    "\n",
    "        # Mant√©m o √∫ltimo ID (√∫ltimo elemento da lista)\n",
    "        base = base.withColumn(\n",
    "            c,\n",
    "            F.when(element_at(F.col(\"_split\"), -1).rlike(\"^[0-9]+$\"),\n",
    "                   element_at(F.col(\"_split\"), -1).cast(\"int\")).otherwise(F.lit(None))\n",
    "        ).drop(\"_split\")\n",
    "\n",
    "    if colunas_virgula:\n",
    "        print(f\"‚úÖ {len(colunas_virgula)} colunas de ID com m√∫ltiplos valores tratadas: {', '.join(colunas_virgula)}\")\n",
    "    else:\n",
    "        print(\"‚úÖ Nenhuma coluna de ID com m√∫ltiplos valores detectada.\")\n",
    "\n",
    "    # 6Ô∏è‚É£ Convers√£o em lote de colunas 'id' e 'posicao_' ‚Üí int\n",
    "    id_cols = [c for c in base.columns if c.lower().startswith((\"id\", \"posicao_\"))]\n",
    "    for c in id_cols:\n",
    "        base = base.withColumn(\n",
    "            c,\n",
    "            F.when(F.col(c).cast(\"int\").isNotNull(), F.col(c).cast(\"int\"))\n",
    "             .otherwise(F.lit(None).cast(\"int\"))\n",
    "        )\n",
    "\n",
    "    # 7Ô∏è‚É£ Metadados\n",
    "    base = base.withColumn(\"data_processamento\",F.from_utc_timestamp(F.current_timestamp(), \"America/Sao_Paulo\"))\n",
    "\n",
    "    # 8Ô∏è‚É£ Reparticionamento l√≥gico (por ID)\n",
    "    base = base.repartition(F.col(coluna_id))\n",
    "\n",
    "    print(\"‚úÖ Fun√ß√£o 'preparar_para_prata' conclu√≠da com sucesso.\")\n",
    "    return base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ae402-7ed7-4388-be1c-809f4dc6e14d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def upsert_prata(\n",
    "    df_final,\n",
    "    path_prata: str,\n",
    "    coluna_id: str,\n",
    "    zorder_cols: list = None\n",
    "):\n",
    "    \"\"\"\n",
    "    PRATA:\n",
    "    - Se df_final tiver `referencia` e `referencia_data`: MERGE incremental por ID + compara√ß√£o de data.\n",
    "    - Se N√ÉO tiver `referencia`/`referencia_data`: FULL REFRESH.\n",
    "    - Cria PRATA se n√£o existir.\n",
    "    - Ajusta propriedades Delta e OPTIMIZE/ZORDER.\n",
    "    - ‚ö†Ô∏è Se o DataFrame estiver vazio, n√£o faz nada.\n",
    "    - üö´ Deduplica automaticamente o df_final pelo ID e maior referencia_data.\n",
    "    \"\"\"\n",
    "    from delta.tables import DeltaTable\n",
    "    from pyspark.sql import functions as F, Window\n",
    "\n",
    "    if df_final is None or df_final.rdd.isEmpty():\n",
    "        print(f\"‚ÑπÔ∏è Nenhum registro novo encontrado. Nenhuma a√ß√£o realizada para {path_prata}.\")\n",
    "        return\n",
    "\n",
    "    cols_l = [c.lower() for c in df_final.columns]\n",
    "    tem_ref = (\"referencia\" in cols_l) and (\"referencia_data\" in cols_l)\n",
    "\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    # üîπ Deduplicar se tiver referencia_data\n",
    "    if tem_ref:\n",
    "        window = Window.partitionBy(coluna_id).orderBy(F.desc(\"referencia_data\"))\n",
    "        df_final = (\n",
    "            df_final.withColumn(\"rn\", F.row_number().over(window))\n",
    "            .filter(F.col(\"rn\") == 1)\n",
    "            .drop(\"rn\")\n",
    "        )\n",
    "        print(f\"üßπ {df_final.count()} registros ap√≥s deduplica√ß√£o por {coluna_id} e referencia_data\")\n",
    "\n",
    "    # PRATA n√£o existe ‚Üí criar (overwrite)\n",
    "    if not DeltaTable.isDeltaTable(spark, path_prata):\n",
    "        print(f\"üÜï Criando PRATA em {path_prata}\")\n",
    "        (\n",
    "            df_final\n",
    "            .coalesce(min(8, max(1, df_final.rdd.getNumPartitions())))\n",
    "            .write.format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "            .save(path_prata)\n",
    "        )\n",
    "    else:\n",
    "        if not tem_ref:\n",
    "            print(f\"üßπ FULL REFRESH da PRATA (sem referencia_*): sobrescrevendo {path_prata}\")\n",
    "            (\n",
    "                df_final\n",
    "                .coalesce(min(8, max(1, df_final.rdd.getNumPartitions())))\n",
    "                .write.format(\"delta\")\n",
    "                .mode(\"overwrite\")\n",
    "                .option(\"overwriteSchema\", \"true\")\n",
    "                .save(path_prata)\n",
    "            )\n",
    "        else:\n",
    "            print(f\"üîÅ MERGE incremental em {path_prata}\")\n",
    "            t = DeltaTable.forPath(spark, path_prata)\n",
    "            merge_condition = f\"t.{coluna_id} = n.{coluna_id}\"\n",
    "\n",
    "            (\n",
    "                t.alias(\"t\")\n",
    "                .merge(df_final.alias(\"n\"), merge_condition)\n",
    "                .whenMatchedUpdateAll(condition=\"t.referencia_data IS NULL OR n.referencia_data > t.referencia_data\")\n",
    "                .whenNotMatchedInsertAll(condition=\"n.referencia IS NOT NULL AND n.referencia_data IS NOT NULL\")\n",
    "                .execute()\n",
    "            )\n",
    "\n",
    "    # Propriedades Delta e otimiza√ß√£o\n",
    "    spark.sql(f\"\"\"\n",
    "        ALTER TABLE delta.`{path_prata}`\n",
    "        SET TBLPROPERTIES (\n",
    "            'delta.enableChangeDataFeed' = 'true',\n",
    "            'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "            'delta.autoOptimize.autoCompact'  = 'true'\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    zcols = zorder_cols or [coluna_id]\n",
    "    print(f\"üß© Otimizando tabela com ZORDER BY ({', '.join(zcols)}) ...\")\n",
    "    spark.sql(f\"OPTIMIZE delta.`{path_prata}` ZORDER BY ({', '.join(zcols)})\")\n",
    "\n",
    "    print(f\"‚úÖ PRATA atualizada e otimizada em {path_prata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb44b37-aa95-4707-b51b-f0a4dfd217ff",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def verificar_ids_com_virgula(df, nome_tabela: str, limite_exemplo: int = 50):\n",
    "    \"\"\"\n",
    "    üîç Verifica colunas que come√ßam com 'id' e detecta registros com m√∫ltiplos IDs separados por v√≠rgula.\n",
    "    ‚úÖ Vers√£o otimizada para Spark / Fabric (executa em apenas 1 job principal)\n",
    "    \n",
    "    Par√¢metros:\n",
    "        df (DataFrame): DataFrame a ser analisado\n",
    "        nome_tabela (str): Nome l√≥gico da tabela (para exibi√ß√£o no log)\n",
    "        limite_exemplo (int): Quantidade de exemplos para exibi√ß√£o (default=50)\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {nome_tabela.upper()} ===\")\n",
    "\n",
    "    # üîπ 1Ô∏è‚É£ Seleciona apenas colunas que come√ßam com 'id'\n",
    "    colunas_id = [c for c in df.columns if c.lower().startswith(\"id\")]\n",
    "\n",
    "    if not colunas_id:\n",
    "        print(\"‚ö†Ô∏è Nenhuma coluna iniciando com 'id' encontrada.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üîé Colunas de ID analisadas: {', '.join(colunas_id)}\")\n",
    "\n",
    "    # üîπ 2Ô∏è‚É£ Cria express√£o √∫nica para todas as colunas com v√≠rgula\n",
    "    condicao_virgula = F.lit(False)\n",
    "    for c in colunas_id:\n",
    "        condicao_virgula = condicao_virgula | F.col(c).cast(\"string\").contains(\",\")\n",
    "\n",
    "    df_com_virgula = df.filter(condicao_virgula)\n",
    "\n",
    "    # üîπ 3Ô∏è‚É£ Agrega contagem por coluna de forma VETORIZADA (sem m√∫ltiplos .count())\n",
    "    exprs_contagem = [\n",
    "        F.sum(F.when(F.col(c).cast(\"string\").contains(\",\"), 1).otherwise(0)).alias(c)\n",
    "        for c in colunas_id\n",
    "    ]\n",
    "    df_contagens = df.agg(*exprs_contagem).first().asDict()\n",
    "\n",
    "    # üîπ 4Ô∏è‚É£ Exibe resultados\n",
    "    encontrou = False\n",
    "    print(\"\\nüìä Colunas com v√≠rgula:\")\n",
    "    for c, qtd in df_contagens.items():\n",
    "        if qtd and qtd > 0:\n",
    "            encontrou = True\n",
    "            print(f\"  ‚Ä¢ {c}: {qtd:,} registros com v√≠rgula\")\n",
    "\n",
    "    if not encontrou:\n",
    "        print(\"‚úÖ Nenhuma coluna de ID cont√©m v√≠rgula.\")\n",
    "        return\n",
    "\n",
    "    # üîπ 5Ô∏è‚É£ Exibe exemplos distintos (limitado)\n",
    "    print(f\"\\nüßæ Exemplo de registros com v√≠rgula (at√© {limite_exemplo}):\")\n",
    "    df_com_virgula.select(*colunas_id).distinct().limit(limite_exemplo).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b955e-5e4e-4729-bd76-6e50347c6956",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "verificar_ids_com_virgula(df_reserva_bronze, \"RESERVA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fda22c-b3f8-435b-842d-129535bc1a6b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "verificar_ids_com_virgula(df_leads_bronze, \"LEAD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b63bcc-ce52-4476-9e34-0da46c0dba2a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "verificar_ids_com_virgula(df_repasse_bronze, \"REPASSE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e295910b-b02b-4e11-9231-a055481f76fd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "verificar_ids_com_virgula(df_precadastro_bronze, \"PRECADASTRO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1bddb-6a0a-4f97-a060-1643f2bd4150",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### [Inserir Lead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fae0b3-78a2-4260-8196-618e0e7e249d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Ex.: LEAD\n",
    "inicio = time.perf_counter()\n",
    "df_leads_prata = preparar_para_prata(df_leads_bronze, coluna_id=\"idlead\", sistema_origem=\"CVCRM\")\n",
    "print(f\"üïê Tempo preparar_para_prata: {time.perf_counter() - inicio:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d8472f-29fb-4e61-a0e9-763c5e5f4c52",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Sanitiza antes do upsert\n",
    "df_leads_prata = sanitize_for_sql_endpoint(df_leads_prata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e5429-b1e2-4f26-a07f-31cc680e760a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df_leads_prata = df_leads_prata.persist()\n",
    "# ou .cache() se n√£o usar StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11b895-4d17-4135-82b4-48f8e6e79b91",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df_leads_prata = df_leads_prata.repartition(4, F.col(\"idlead\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b6d9a-bfe2-472b-afc8-9625ce631f15",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"d8feb57f-31e9-473a-ab89-ab0dbf7269d4\",\"activityId\":\"f8017210-bb1b-41d4-a7f9-86e5c9392bf1\",\"applicationId\":\"application_1764607245561_0001\",\"jobGroupId\":\"45\",\"advices\":{\"warn\":1}}"
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "inicio = time.perf_counter()\n",
    "upsert_prata(df_leads_prata, f\"{prata_lakehouse_path}/lead_prata\", coluna_id=\"idlead\", zorder_cols=[\"idlead\",\"referencia_data\"])\n",
    "print(f\"üïê Tempo upsert_prata: {time.perf_counter() - inicio:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e44289-8b10-46a2-b72f-c655ed119c6c",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"d8feb57f-31e9-473a-ab89-ab0dbf7269d4\",\"activityId\":\"644995e1-d446-48d7-8180-e4c2d928fca7\",\"applicationId\":\"application_1763402579437_0001\",\"jobGroupId\":\"34\",\"advices\":{\"warn\":1}}"
    },
    "editable": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"OPTIMIZE delta.`{prata_lakehouse_path}/lead_prata` ZORDER BY (idlead)\")\n",
    "spark.sql(f\"VACUUM delta.`{prata_lakehouse_path}/lead_prata` RETAIN 168  HOURS\")  # mant√©m 7 dias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e6f43b-e136-431d-aafb-91a07c5594a6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### [Inserir Precadastro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7940b069-660e-4c3a-88ff-c468cbf7b5bb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "inicio = time.perf_counter()\n",
    "df_precadastro_prata = preparar_para_prata(\n",
    "    df_precadastro_bronze,\n",
    "    coluna_id=\"idprecadastro\",\n",
    "    sistema_origem=\"CVCRM\"\n",
    ")\n",
    "print(f\"üïê Tempo preparar_para_prata (precadastro): {time.perf_counter() - inicio:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c285135-54b2-4ab3-9785-565e135fed89",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Sanitiza antes do upsert\n",
    "df_precadastro_prata = sanitize_for_sql_endpoint(df_precadastro_prata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f24635-5c78-41ab-b2b1-f387981eb19d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# üîπ Persist para evitar reprocessamento\n",
    "df_precadastro_prata = df_precadastro_prata.persist()\n",
    "# üîπ Particionamento controlado ‚Äî base moderada\n",
    "df_precadastro_prata = df_precadastro_prata.repartition(4, F.col(\"idprecadastro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226cc42e-42dc-4761-95ca-4e8cc21cd876",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# üîπ Upsert otimizado\n",
    "inicio = time.perf_counter()\n",
    "upsert_prata(\n",
    "    df_precadastro_prata,\n",
    "    f\"{prata_lakehouse_path}/precadastro_prata\",\n",
    "    coluna_id=\"idprecadastro\",\n",
    "    zorder_cols=[\"idprecadastro\", \"referencia_data\"]\n",
    ")\n",
    "print(f\"üïê Tempo upsert_prata (precadastro): {time.perf_counter() - inicio:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6154178-04c4-4a54-9486-6148e182e4b3",
   "metadata": {
    "editable": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# üîπ Compacta√ß√£o e limpeza\n",
    "spark.sql(f\"OPTIMIZE delta.`{prata_lakehouse_path}/precadastro_prata` ZORDER BY (idprecadastro)\")\n",
    "spark.sql(f\"VACUUM delta.`{prata_lakehouse_path}/precadastro_prata` RETAIN 168 HOURS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca874c2-e9ee-4a2e-a538-5ee389e642b5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### [Inserir Reserva]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29171de-8a3c-40e9-8620-fae16c3e1bb3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- RESERVA -------------------\n",
    "inicio = time.perf_counter()\n",
    "df_reserva_prata = preparar_para_prata(\n",
    "    df_reserva_bronze,\n",
    "    coluna_id=\"idreserva\",\n",
    "    sistema_origem=\"CVCRM\"\n",
    ")\n",
    "print(f\"üïê Tempo preparar_para_prata (reserva): {time.perf_counter() - inicio:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81bb7ce-cb6f-47ef-9cba-c095573d8fe3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Sanitiza antes do upsert\n",
    "df_reserva_prata = sanitize_for_sql_endpoint(df_reserva_prata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6394280-762b-4b41-a0d6-92b8c2e22de4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df_reserva_prata = df_reserva_prata.persist()\n",
    "\n",
    "# üîπ Base maior ‚Üí mais parti√ß√µes\n",
    "df_reserva_prata = df_reserva_prata.repartition(8, F.col(\"idreserva\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fac666-91c5-4bcb-8ed4-458afc88ded2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df_reserva_prata.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f09957-8a70-4e1b-9709-6f50dd728817",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "inicio = time.perf_counter()\n",
    "upsert_prata(\n",
    "    df_reserva_prata,\n",
    "    f\"{prata_lakehouse_path}/reserva_prata\",\n",
    "    coluna_id=\"idreserva\",\n",
    "    zorder_cols=[\"idreserva\", \"referencia_data\"]\n",
    ")\n",
    "print(f\"üïê Tempo upsert_prata (reserva): {time.perf_counter() - inicio:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b3e202-a540-4510-a927-af4c473f517b",
   "metadata": {
    "editable": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"OPTIMIZE delta.`{prata_lakehouse_path}/reserva_prata` ZORDER BY (idreserva)\")\n",
    "spark.sql(f\"VACUUM delta.`{prata_lakehouse_path}/reserva_prata` RETAIN 168 HOURS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd6dea-86ab-41f3-9598-5f05e922d350",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "###### [Reserva]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3583be76-c2af-4b0b-9913-cb2a9b4c858c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- REPASSE -------------------\n",
    "inicio = time.perf_counter()\n",
    "df_repasse_prata = preparar_para_prata(\n",
    "    df_repasse_bronze,\n",
    "    coluna_id=\"idrepasse\",\n",
    "    sistema_origem=\"CVCRM\"\n",
    ")\n",
    "print(f\"üïê Tempo preparar_para_prata (repasse): {time.perf_counter() - inicio:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba1a7f-ca95-46a7-8172-6dae6bf223aa",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Sanitiza antes do upsert\n",
    "df_repasse_prata = sanitize_for_sql_endpoint(df_repasse_prata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed8780-a2a8-4a00-985d-d7cc0d964769",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df_repasse_prata = df_repasse_prata.persist()\n",
    "\n",
    "# üîπ Base geralmente menor ‚Üí menos parti√ß√µes\n",
    "df_repasse_prata = df_repasse_prata.repartition(4, F.col(\"idrepasse\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53aaa4-4cb8-457d-94cc-46e185db2384",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "inicio = time.perf_counter()\n",
    "upsert_prata(\n",
    "    df_repasse_prata,\n",
    "    f\"{prata_lakehouse_path}/repasse_prata\",\n",
    "    coluna_id=\"idrepasse\",\n",
    "    zorder_cols=[\"idrepasse\", \"referencia_data\"]\n",
    ")\n",
    "print(f\"üïê Tempo upsert_prata (repasse): {time.perf_counter() - inicio:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b650def4-6417-4909-9803-3e739f030c17",
   "metadata": {
    "editable": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"OPTIMIZE delta.`{prata_lakehouse_path}/repasse_prata` ZORDER BY (idrepasse)\")\n",
    "spark.sql(f\"VACUUM delta.`{prata_lakehouse_path}/repasse_prata` RETAIN 168 HOURS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d31d6da-8fce-430a-a49d-db1893490c10",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## [**Tratar Campos Adicionais Tratamento**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c61cea-82b6-4bc2-b7e0-d3dff15c2ae0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "path_campos_bronze = f\"{bronze_lakehouse_path}/campos_adicionais_bronze_cvdw\"\n",
    "path_campos_prata  = f\"{prata_lakehouse_path}/campos_adicionais_prata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8c733-92d8-4561-8ca3-b69828217338",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "spark.conf.set(\"spark.sql.parquet.outputTimestampType\", \"TIMESTAMP_MICROS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c788f56-c046-439c-a02f-74df8fd24562",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "EXCLUSOES_CAMPOS = {\n",
    "    \"lead\": {\n",
    "        \"excluir_valor\": [\n",
    "            \"lead_categoria_de_renda_parceiro_conjuge\",\n",
    "            \"lead_categoria_de_renda\",\n",
    "            \"lead_descricao_subsidio_price\",\n",
    "            \"lead_declarou_imposto_de_renda\",\n",
    "            \"lead_diferenca_da_renta_atual_para_a_renda_ideal_price\",\n",
    "            \"lead_diferenca_da_renta_atual_para_a_renda_ideal_sac\",\n",
    "            \"lead_link_resultados_price_renda_ideal\",\n",
    "            \"lead_link_resultados_sac_renda_ideal\",\n",
    "            \"lead_modalidade_de_financiamento_simulacao\",\n",
    "            \"lead_descricao_subsidio_sac\",\n",
    "            \"lead_possui_3_anos_sob_regime_do_fgts\",\n",
    "            \"lead_renda_ideal_r_i_para_price\",\n",
    "            \"lead_renda_ideal_r_i_para_sac\",\n",
    "            \"lead_renda_parceiro_conjuge\",\n",
    "            \"lead_possui_fgts\",\n",
    "            \"lead_quantos_anos_de_fgts\",\n",
    "            \"lead_renda_principal\",\n",
    "            \"lead_faixa_de_valor\"\n",
    "        ],\n",
    "        \"forcar_valor\": [\n",
    "            \"lead_restricao_lead\"\n",
    "        ],\n",
    "        \"excluir_data\": []\n",
    "    },\n",
    "\n",
    "    \"precadastro\": {\n",
    "        \"excluir_valor\": [\n",
    "            \"precadastro_3_anos_de_fgts\",\n",
    "            \"precadastro_avaliacao_de_risco_com_quantos_participantes_com_renda\",\n",
    "            \"precadastro_categoria_de_renda\",\n",
    "            \"precadastro_modalidade_de_financiamento\",\n",
    "            \"precadastro_possui_3_anos_sob_regime_do_fgts\",\n",
    "            \"precadastro_tipo_da_renda\"\n",
    "        ],\n",
    "        \"forcar_valor\": [\n",
    "            \"precadastro_valor_de_financiamento_apenas_fi\",\n",
    "            \"precadastro_valor_de_laudo_do_imovel_pretendido\",\n",
    "            \"precadastro_valor_do_subsidio_concedido_cef\",\n",
    "            \"precadastro_valor_itbi\",\n",
    "            \"precadastro_renda_bruta\",\n",
    "            \"precadastro_renda_bruta_formal\",\n",
    "            \"precadastro_renda_informal\",\n",
    "            \"precadastro_renda_liquida\"\n",
    "        ],\n",
    "        \"forcar_data\": [],\n",
    "        \"excluir_data\": [\n",
    "            \"precadastro_motivo_da_analise\",\n",
    "            \"precadastro_profissao_da_analise_de_credito\",\n",
    "            \"precadastro_resposta_do_perfil\",\n",
    "            \"precadastro_resultado_da_analise\",\n",
    "            \"precadastro_3_anos_de_fgts\",\n",
    "            \"precadastro_aguardando_qv\",\n",
    "            \"precadastro_avaliacao_de_risco_com_quantos_participantes_com_renda\",\n",
    "            \"precadastro_cadastro_bloqueado\",\n",
    "            \"precadastro_cadastro_irregular_na_receita\",\n",
    "            \"precadastro_categoria_de_renda\",\n",
    "            \"precadastro_documentacao_pendente\",\n",
    "            \"precadastro_e_seu_primeiro_imovel\",\n",
    "            \"precadastro_erro_cadastral\",\n",
    "            \"precadastro_estado_civil\",\n",
    "            \"precadastro_formacao\",\n",
    "            \"precadastro_justificativa_nao_venda\",\n",
    "            \"precadastro_modalidade_de_financiamento\",\n",
    "            \"precadastro_momento_do_pre_cadastro\",\n",
    "            \"precadastro_numero_de_qv_disponivel\",\n",
    "            \"precadastro_observacao\",\n",
    "            \"precadastro_onde_reside\",\n",
    "            \"precadastro_parcela_aprovada\",\n",
    "            \"precadastro_pasta_duplicada\",\n",
    "            \"precadastro_possibilidade_de_venda\",\n",
    "            \"precadastro_possui_3_anos_sob_regime_do_fgts\",\n",
    "            \"precadastro_possui_dependente\",\n",
    "            \"precadastro_processo_a_intenalizar_agencia\",\n",
    "            \"precadastro_processo_intenalizado_agencia\",\n",
    "            \"precadastro_profissao_1\",\n",
    "            \"precadastro_restricao_1\",\n",
    "            \"precadastro_restricao_2\",\n",
    "            \"precadastro_sexo\",\n",
    "            \"precadastro_status_da_negociacao\",\n",
    "            \"precadastro_sub_fase\",\n",
    "            \"precadastro_tipo_da_renda\",\n",
    "            \"precadastro_tipo_de_dependente\",\n",
    "            'precadastro_desistente', 'precadastro_renda_bruta', 'precadastro_renda_bruta_formal', 'precadastro_renda_informal', 'precadastro_renda_liquida', 'precadastro_valor_de_financiamento_apenas_fi', 'precadastro_valor_de_laudo_do_imovel_pretendido', 'precadastro_valor_do_subsidio_concedido_cef', 'precadastro_valor_itbi',\n",
    "            \"precadastro_profissao_da_analise_de_credito\"\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    \"reserva\": {\n",
    "        \"excluir_valor\": [\"reserva_categoria_de_renda_do_fiador\"],\n",
    "        \"excluir_data\": [\"reserva_data_do_kit_registro_ok\"]\n",
    "    },\n",
    "\n",
    "    \"repasse\": {\n",
    "        \"excluir_valor\": [\n",
    "            \"repasse_ap_itbi\",\n",
    "            \"repasse_fgts_futuro\",\n",
    "            \"repasse_protocolo_itbi\",\n",
    "            \"repasse_valor_do_contrato_7lm\",\n",
    "            \"repasse_valor_do_contrato_caixa\",\n",
    "            \"repasse_valor_do_produto_caixa\",\n",
    "            \"repasse_valor_itbi\",\n",
    "            \"repasse_classificacao_preco\"\n",
    "        ],\n",
    "        \"excluir_data\": [\n",
    "            \"repasse_data_de_solicitacao_do_itbi_prefeitura\",\n",
    "            \"repasse_data_pagamento_do_itbi\",\n",
    "            \"repasse_data_solicitacao_pagamento_itbi_financeiro\",\n",
    "            \"repasse_pendencia_de_repasse_descricao_e_data\",\n",
    "            \"repasse_previsao_de_data_de_saida_do_cartorio\",\n",
    "            \"repasse_probabilidade_de_assinatura\"\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    \"distrato\": {\"excluir_valor\": [], \"excluir_data\": []}\n",
    "}\n",
    "\n",
    "CHAVES_GRUPO = [\"idfuncionalidade\", \"funcionalidade\", \"referencia\", \"referencia_data\"]\n",
    "\n",
    "COLLISION_OVERRIDES = {\n",
    "    \"lead\": {\n",
    "        \"Primeiro Im√≥vel?\": \"primeiro_imovel_1\",\n",
    "        \"primeiro_imovel\": \"primeiro_imovel_2\",\n",
    "        \"Regi√£o de Interesse\": \"regiao_de_interesse_1\",\n",
    "        \"Regi√£o de interesse\": \"regiao_de_interesse_2\",\n",
    "    },\n",
    "    \"precadastro\": {\n",
    "        \"Restri√ß√£o\": \"restricao_1\",\n",
    "        \"Restri√ß√£o?\": \"restricao_2\",\n",
    "    },\n",
    "    \"repasse\": {\n",
    "        \"Obs Garantia\": \"obs_garantia_1\",\n",
    "        \"Obs Garantia:\": \"obs_garantia_2\",\n",
    "    },\n",
    "}\n",
    "\n",
    "PATHS_MAIN_PRATA = {\n",
    "    \"lead\":        f\"{prata_lakehouse_path}/lead_prata\",\n",
    "    \"precadastro\": f\"{prata_lakehouse_path}/precadastro_prata\",\n",
    "    \"repasse\":     f\"{prata_lakehouse_path}/repasse_prata\",\n",
    "    \"reserva\":     f\"{prata_lakehouse_path}/reserva_prata\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7dc40f-8d97-49dc-8114-7c289f013b7d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def _norm_value(v: str):\n",
    "    \"\"\"Normaliza valores textuais, removendo aspas externas, estruturas serializadas e strings vazias.\"\"\"\n",
    "    if v is None:\n",
    "        return None\n",
    "\n",
    "    v = str(v).strip()\n",
    "\n",
    "    # Remove aspas externas diretas: \"SIM\" ‚Üí SIM\n",
    "    if len(v) >= 2 and v.startswith('\"') and v.endswith('\"'):\n",
    "        v = v[1:-1].strip()\n",
    "\n",
    "    # Captura o √∫ltimo valor entre aspas dentro de estruturas serializadas\n",
    "    # Ex: a:1:{s:0:\"SIM\";} -> SIM\n",
    "    m = re.findall(r'\"([^\"]+)\"', v)\n",
    "    if m:\n",
    "        return m[-1].strip()\n",
    "\n",
    "    # Se estiver vazio, retorna None\n",
    "    if v.strip() == \"\":\n",
    "        return None\n",
    "\n",
    "    return v\n",
    "\n",
    "def _norm_basic(s: str) -> str:\n",
    "    \"\"\"Normaliza nomes removendo acentos, aspas, espa√ßos e caracteres especiais.\"\"\"\n",
    "    s = str(s).replace('\"', '').replace(\"'\", \"\")\n",
    "    s2 = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    s2 = re.sub(r\"[^A-Za-z0-9_]+\", \"_\", s2.strip().lower())\n",
    "    s2 = re.sub(r\"_+\", \"_\", s2).strip(\"_\")\n",
    "    return s2 or \"col\"\n",
    "\n",
    "\n",
    "\n",
    "def fix_duplicate_columns(df, label: str = \"\"):\n",
    "    \"\"\"Corrige nomes de colunas duplicadas dentro do mesmo DF.\"\"\"\n",
    "    cols = df.columns\n",
    "    cont = Counter(cols)\n",
    "\n",
    "    if all(v == 1 for v in cont.values()):\n",
    "        print(f\"‚úÖ [{label}] Nenhuma coluna duplicada interna para corrigir.\")\n",
    "        return df\n",
    "\n",
    "    seen = {}\n",
    "    for c in cols:\n",
    "        seen[c] = seen.get(c, 0) + 1\n",
    "        if seen[c] > 1:\n",
    "            new_name = f\"{c}__fix{seen[c]}\"\n",
    "            print(f\"   - Renomeado: {c} -> {new_name}\")\n",
    "            df = df.withColumnRenamed(c, new_name)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607d901-d42c-4426-8a06-1dab4be2dc95",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def _build_safe_mapping_for_pivot(df_func, func_label):\n",
    "    \"\"\"Gera mapeamento seguro de campo_nome ‚Üí campo_nome_safe considerando overrides.\"\"\"\n",
    "    raw_vals = [r[0] for r in df_func.select(\"campo_nome\").distinct().collect()]\n",
    "    overrides = COLLISION_OVERRIDES.get(func_label, {})\n",
    "\n",
    "    used = defaultdict(int)\n",
    "    mapping_rows, listed_overrides = [], []\n",
    "\n",
    "    # aplica overrides\n",
    "    for rv in raw_vals:\n",
    "        if rv in overrides:\n",
    "            mapping_rows.append((rv, overrides[rv]))\n",
    "            listed_overrides.append(rv)\n",
    "\n",
    "    # normaliza√ß√£o padr√£o\n",
    "    for rv in raw_vals:\n",
    "        if rv in listed_overrides:\n",
    "            continue\n",
    "\n",
    "        rv_clean = str(rv).replace('\"', '').replace(\"'\", \"\").strip()\n",
    "        base = _norm_basic(rv_clean)\n",
    "\n",
    "        used[base] += 1\n",
    "        idx = used[base]\n",
    "        safe = base if idx == 1 else f\"{base}__pvdup{idx}\"\n",
    "\n",
    "        mapping_rows.append((rv, safe))\n",
    "\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"campo_nome\", T.StringType(), True),\n",
    "        T.StructField(\"campo_nome_safe\", T.StringType(), True),\n",
    "    ])\n",
    "\n",
    "    return spark.createDataFrame(mapping_rows, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e464c46f-40fb-4f39-aab1-7ba46b5bd0d4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def _pivotar_com_prefixo_debug(df_bronze_inc):\n",
    "    \"\"\"Pivota√ß√£o segura, prefixos e consolida√ß√£o por funcionalidade.\"\"\"\n",
    "\n",
    "    # ===============================================================\n",
    "    # 1Ô∏è‚É£ NORMALIZA√á√ÉO DE VALORES (ANTES DO PIVOT)  <<<< ESTE √â O PASSO 5\n",
    "    # ===============================================================\n",
    "    df_bronze_inc = df_bronze_inc.withColumn(\n",
    "        \"valor\", F.udf(_norm_value, T.StringType())(F.col(\"valor\"))\n",
    "    )\n",
    "    print(\"üßπ Valores normalizados antes do pivot.\")\n",
    "\n",
    "    # ===============================================================\n",
    "    # Identifica funcionalidades existentes\n",
    "    # ===============================================================\n",
    "    funcionalidades = (\n",
    "        df_bronze_inc.select(\"funcionalidade\")\n",
    "        .distinct()\n",
    "        .rdd.flatMap(lambda x: x)\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    dfs_tratados = []\n",
    "\n",
    "    # ===============================================================\n",
    "    # Processa cada funcionalidade isoladamente\n",
    "    # ===============================================================\n",
    "    for func in funcionalidades:\n",
    "        print(f\"üìä Pivotando funcionalidade: {func}\")\n",
    "\n",
    "        df_func = df_bronze_inc.filter(F.col(\"funcionalidade\") == func)\n",
    "\n",
    "        # ===============================================================\n",
    "        # 2Ô∏è‚É£ Gera mapeamento seguro de nomes de campo\n",
    "        # ===============================================================\n",
    "        map_df = _build_safe_mapping_for_pivot(df_func, func)\n",
    "\n",
    "        df_func_safe = (\n",
    "            df_func.join(F.broadcast(map_df), on=\"campo_nome\", how=\"left\")\n",
    "                   .withColumn(\"campo_nome_safe\",\n",
    "                               F.coalesce(F.col(\"campo_nome_safe\"), F.lit(\"col\")))\n",
    "        )\n",
    "\n",
    "        # ===============================================================\n",
    "        # 3Ô∏è‚É£ Pivot real\n",
    "        # ===============================================================\n",
    "        df_f = (\n",
    "            df_func_safe\n",
    "              .groupBy(*CHAVES_GRUPO)\n",
    "              .pivot(\"campo_nome_safe\")\n",
    "              .agg(F.first(\"valor\"))\n",
    "        )\n",
    "\n",
    "        # ===============================================================\n",
    "        # 4Ô∏è‚É£ Prefixa colunas baseadas na funcionalidade\n",
    "        # ===============================================================\n",
    "        chaves_set = set(CHAVES_GRUPO)\n",
    "        alias_exprs = [F.col(k) for k in CHAVES_GRUPO]\n",
    "\n",
    "        for c in [c for c in df_f.columns if c not in chaves_set]:\n",
    "            alias_exprs.append(F.col(f\"`{c}`\").alias(f\"{func}_{c}\"))\n",
    "\n",
    "        df_alias = df_f.select(*alias_exprs)\n",
    "\n",
    "        # ===============================================================\n",
    "        # 5Ô∏è‚É£ Remove colunas duplicadas internas\n",
    "        # ===============================================================\n",
    "        df_clean = fix_duplicate_columns(df_alias, label=func)\n",
    "\n",
    "        # ===============================================================\n",
    "        # 6Ô∏è‚É£ Consolida√ß√£o final por idfuncionalidade + funcionalidade\n",
    "        # ===============================================================\n",
    "        id_col = \"idfuncionalidade\"\n",
    "\n",
    "        non_key_cols = [c for c in df_clean.columns if c not in CHAVES_GRUPO]\n",
    "\n",
    "        agg_exprs = [\n",
    "            F.first(F.when(F.col(c).isNotNull(), F.col(c)), ignorenulls=True).alias(c)\n",
    "            for c in non_key_cols\n",
    "        ]\n",
    "\n",
    "        df_consolidado = (\n",
    "            df_clean\n",
    "            .groupBy(id_col, \"funcionalidade\")\n",
    "            .agg(\n",
    "                F.max(\"referencia\").alias(\"referencia\"),\n",
    "                F.max(\"referencia_data\").alias(\"referencia_data\"),\n",
    "                *agg_exprs\n",
    "            )\n",
    "        )\n",
    "\n",
    "        dfs_tratados.append(df_consolidado)\n",
    "\n",
    "    # ===============================================================\n",
    "    # 7Ô∏è‚É£ Uni√£o final com alinhamento de colunas\n",
    "    # ===============================================================\n",
    "    all_cols = sorted(set().union(*[set(d.columns) for d in dfs_tratados]))\n",
    "\n",
    "    ordered_cols = (\n",
    "        [c for c in CHAVES_GRUPO if c in all_cols] +\n",
    "        [c for c in all_cols if c not in CHAVES_GRUPO]\n",
    "    )\n",
    "\n",
    "    def _sel(d): \n",
    "        return [c for c in ordered_cols if c in d.columns]\n",
    "\n",
    "    df_final = dfs_tratados[0].select(_sel(dfs_tratados[0]))\n",
    "\n",
    "    for d in dfs_tratados[1:]:\n",
    "        df_final = df_final.unionByName(d.select(_sel(d)), allowMissingColumns=True)\n",
    "\n",
    "    print(f\"‚úÖ Pivota√ß√£o conclu√≠da e consolidada: {df_final.count()} linhas / {len(df_final.columns)} colunas.\")\n",
    "    return df_final, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd32056-ab57-4423-aa62-4deead5d7f0a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def ler_incremental_bronze_campos(path_bronze: str, path_prata: str, funcionalidade: str):\n",
    "    \"\"\"\n",
    "    Retorna (df_bronze_inc, full_load: bool, ultima_ref).\n",
    "    Faz leitura incremental da Bronze considerando a √∫ltima referencia_data\n",
    "    da FUNCIONALIDADE espec√≠fica j√° existente na PRATA (campos adicionais).\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîé Lendo Bronze de forma incremental... (funcionalidade: {funcionalidade})\")\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    # Se PRATA (campos) n√£o existe ‚Üí full load daquela funcionalidade\n",
    "    if not DeltaTable.isDeltaTable(spark, path_prata):\n",
    "        print(\"üÜï Nenhuma PRATA de Campos Adicionais ‚Äî carga completa da Bronze.\")\n",
    "        df_bronze_inc = (\n",
    "            spark.read.format(\"delta\").load(path_bronze)\n",
    "                 .filter(F.col(\"funcionalidade\") == funcionalidade)\n",
    "        )\n",
    "        return df_bronze_inc, True, None\n",
    "\n",
    "    # √öltima refer√™ncia j√° processada na PRATA (campos) para a funcionalidade\n",
    "    df_prata = spark.read.format(\"delta\").load(path_prata)\n",
    "    ultima_ref_row = (\n",
    "        df_prata.filter(F.col(\"funcionalidade\") == funcionalidade)\n",
    "                .select(F.max(\"referencia_data\").alias(\"ultima_ref\"))\n",
    "                .first()\n",
    "    )\n",
    "    ultima_ref = ultima_ref_row[\"ultima_ref\"] if ultima_ref_row else None\n",
    "    print(f\"üìÖ √öltima referencia_data (na PRATA - {funcionalidade}): {ultima_ref}\")\n",
    "\n",
    "    # Se n√£o tem hist√≥rico na PRATA ‚Üí full load\n",
    "    if not ultima_ref:\n",
    "        print(f\"üÜï Nenhuma refer√™ncia anterior para {funcionalidade} ‚Äî carga completa.\")\n",
    "        df_bronze_inc = (\n",
    "            spark.read.format(\"delta\").load(path_bronze)\n",
    "                 .filter(F.col(\"funcionalidade\") == funcionalidade)\n",
    "        )\n",
    "        return df_bronze_inc, True, None\n",
    "\n",
    "    # Incremental via CDF (fallback por referencia_data)\n",
    "    try:\n",
    "        df_bronze_inc = (\n",
    "            spark.read.format(\"delta\")\n",
    "                 .option(\"readChangeFeed\", \"true\")\n",
    "                 .option(\"startingTimestamp\", ultima_ref)\n",
    "                 .load(path_bronze)\n",
    "                 .filter(\n",
    "                     (F.col(\"_change_type\").isin(\"insert\", \"update_postimage\")) &\n",
    "                     (F.col(\"funcionalidade\") == funcionalidade)\n",
    "                 )\n",
    "                 .drop(\"_change_type\", \"_commit_version\", \"_commit_timestamp\")\n",
    "        )\n",
    "        print(f\"‚ö° Incremental via Change Data Feed (CDF) para {funcionalidade}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è CDF indispon√≠vel ‚Äî fallback por referencia_data. ({e})\")\n",
    "        df_bronze_inc = (\n",
    "            spark.read.format(\"delta\").load(path_bronze)\n",
    "                 .filter(\n",
    "                     (F.col(\"referencia_data\") > F.lit(ultima_ref)) &\n",
    "                     (F.col(\"funcionalidade\") == funcionalidade)\n",
    "                 )\n",
    "        )\n",
    "    return df_bronze_inc, False, ultima_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16766b35-6e33-419b-95b2-33a57fa9728f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def _normalizar_exclusoes(funcionalidade: str, cols_presentes: list):\n",
    "    \"\"\"\n",
    "    Identifica colunas que N√ÉO devem ser tratadas como valor/data\n",
    "    (mas permanecem no DF final).\n",
    "    \"\"\"\n",
    "    cfg = EXCLUSOES_CAMPOS.get(funcionalidade, {})\n",
    "    excluir_valor = set(cfg.get(\"excluir_valor\", []))\n",
    "    excluir_data  = set(cfg.get(\"excluir_data\", []))\n",
    "    forcar_valor  = set(cfg.get(\"forcar_valor\", []))\n",
    "    forcar_data   = set(cfg.get(\"forcar_data\", []))\n",
    "\n",
    "    existentes = set(cols_presentes)\n",
    "    validas_excluir_valor = sorted(list(excluir_valor & existentes))\n",
    "    validas_excluir_data  = sorted(list(excluir_data  & existentes))\n",
    "    validas_forcar_valor  = sorted(list(forcar_valor & existentes))\n",
    "    validas_forcar_data   = sorted(list(forcar_data  & existentes))\n",
    "\n",
    "    ausentes = sorted(list((excluir_valor | excluir_data | forcar_valor | forcar_data) - existentes))\n",
    "    if ausentes:\n",
    "        print(f\"‚ÑπÔ∏è Colunas configuradas mas ausentes no schema ({funcionalidade}): {', '.join(ausentes)}\")\n",
    "\n",
    "    return {\n",
    "        \"excluir_valor\": validas_excluir_valor,  # N√ÉO tratar como valor\n",
    "        \"excluir_data\":  validas_excluir_data,   # N√ÉO tratar como data\n",
    "        \"forcar_valor\":  validas_forcar_valor,\n",
    "        \"forcar_data\":   validas_forcar_data,\n",
    "    }\n",
    "\n",
    "def _cast_valor(col):\n",
    "    \"\"\"\n",
    "    Converte strings tipo BR para n√∫mero:\n",
    "    - remove 'R$' e texto\n",
    "    - remove separador de milhar '.'\n",
    "    - converte v√≠rgula para ponto\n",
    "    - par√™nteses = negativo\n",
    "    \"\"\"\n",
    "    s = F.col(col).cast(\"string\")\n",
    "    sign = F.when(s.rlike(r\"\\(\"), F.lit(-1.0)) \\\n",
    "            .when(s.startswith(\"-\"), -1.0) \\\n",
    "            .otherwise(1.0)\n",
    "    cleaned = F.regexp_replace(s, r\"[^\\d,.\\-()]\", \"\")\n",
    "    cleaned = F.regexp_replace(cleaned, r\"\\.\", \"\")\n",
    "    cleaned = F.regexp_replace(cleaned, r\",\", \".\")\n",
    "    num = cleaned.cast(T.DoubleType())\n",
    "    return (sign * num)\n",
    "\n",
    "def _cast_timestamp_multi(col):\n",
    "    \"\"\"Tenta m√∫ltiplos formatos comuns (BR e ISO).\"\"\"\n",
    "    c = F.col(col).cast(\"string\")\n",
    "    return F.coalesce(\n",
    "        F.to_timestamp(c, \"yyyy-MM-dd HH:mm:ss\"),\n",
    "        F.to_timestamp(c, \"yyyy-MM-dd'T'HH:mm:ss\"),\n",
    "        F.to_timestamp(c, \"dd/MM/yyyy HH:mm:ss\"),\n",
    "        F.to_timestamp(c, \"dd/MM/yyyy\"),\n",
    "        F.to_timestamp(c, \"yyyy-MM-dd\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabaec4f-7612-4e2c-9ab4-e6487eae5e1c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def tratar_funcionalidade(df_pivot, funcionalidade: str):\n",
    "    f = funcionalidade\n",
    "    prefix = f + \"_\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(f\"üß© INICIANDO TRATAMENTO: {f.upper()}\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # mant√©m TODAS as colunas da funcionalidade + chaves\n",
    "    cols_func = [c for c in df_pivot.columns if c.startswith(prefix)] + [c for c in CHAVES_GRUPO if c in df_pivot.columns]\n",
    "    df_f = df_pivot.select(*cols_func).filter(F.col(\"funcionalidade\") == f)\n",
    "\n",
    "    print(f\"üìä Total de colunas encontradas ({f}): {len(df_f.columns)}\")\n",
    "    print(f\"üîë Chaves presentes: {[k for k in CHAVES_GRUPO if k in df_f.columns]}\")\n",
    "\n",
    "    # 1) l√™ as listas de exce√ß√£o/for√ßa e valida com schema\n",
    "    cfg_ex = _normalizar_exclusoes(f, df_f.columns)\n",
    "    colunas_excluir_valor = cfg_ex[\"excluir_valor\"]  # N√ÉO tratar como valor\n",
    "    colunas_excluir_data  = cfg_ex[\"excluir_data\"]   # N√ÉO tratar como data\n",
    "    colunas_forcar_valor  = cfg_ex[\"forcar_valor\"]\n",
    "    colunas_forcar_data   = cfg_ex[\"forcar_data\"]\n",
    "\n",
    "    print(f\"üö´ N√ÉO tratar como VALOR: {', '.join(colunas_excluir_valor) if colunas_excluir_valor else '‚Äî'}\")\n",
    "    print(f\"üö´ N√ÉO tratar como DATA:  {', '.join(colunas_excluir_data)  if colunas_excluir_data  else '‚Äî'}\")\n",
    "    print(f\"üí™ For√ßar VALOR: {', '.join(colunas_forcar_valor) if colunas_forcar_valor else '‚Äî'}\")\n",
    "    print(f\"üìÖ For√ßar DATA:  {', '.join(colunas_forcar_data)  if colunas_forcar_data  else '‚Äî'}\")\n",
    "\n",
    "    # ‚ùó N√£o removemos nada: tratamos por cima de df_f\n",
    "    df_para_tratar = df_f\n",
    "\n",
    "    # 2) detec√ß√£o autom√°tica de candidatos (pelo nome da coluna sem o prefixo)\n",
    "    palavras_valor = [\"valor\", \"renda\", \"preco\", \"pre√ßo\", \"subsidio\", \"financiamento\", \"fgts\", \"itbi\", \"laudo\", \"price\", \"sac\"]\n",
    "    palavras_data  = [\"data\", \"nascimento\", \"envio\", \"assinatura\", \"vencimento\", \"cadastro\", \"alteracao\", \"atualizacao\", \"emissao\"]\n",
    "\n",
    "    def strip_prefix(c: str):\n",
    "        return c[len(prefix):] if c.startswith(prefix) else c\n",
    "\n",
    "    colunas_valor_auto = [c for c in df_para_tratar.columns if any(p in strip_prefix(c).lower() for p in palavras_valor)]\n",
    "    colunas_data_auto  = [c for c in df_para_tratar.columns if any(p in strip_prefix(c).lower() for p in palavras_data)]\n",
    "\n",
    "    # 3) combina: (auto ‚à™ for√ßar) ‚àí excluir  ‚Üí listas finais a tratar\n",
    "    colunas_valor = sorted( (set(colunas_valor_auto) | set(colunas_forcar_valor)) - set(colunas_excluir_valor) )\n",
    "    colunas_data  = sorted( (set(colunas_data_auto)  | set(colunas_forcar_data))  - set(colunas_excluir_data) )\n",
    "\n",
    "    print(f\"üí∞ Tratamento VALOR ({len(colunas_valor)}): {', '.join(colunas_valor) if colunas_valor else '‚Äî'}\")\n",
    "    print(f\"üìÖ Tratamento DATA  ({len(colunas_data)}): {', '.join(colunas_data)  if colunas_data  else '‚Äî'}\")\n",
    "\n",
    "    # 4) chama o tratador passando as listas de N√ÉO-TRATAR\n",
    "    df_tratado = tratar_valores_e_datas(\n",
    "        df_para_tratar,\n",
    "        colunas_valor=colunas_valor,          # j√° SEM as exclu√≠das\n",
    "        colunas_data=colunas_data,            # j√° SEM as exclu√≠das\n",
    "        excluir_valor=colunas_excluir_valor,  # mant√©m no DF, s√≥ n√£o trata\n",
    "        excluir_data=colunas_excluir_data,    # mant√©m no DF, s√≥ n√£o trata\n",
    "        forcar_valor=colunas_forcar_valor,\n",
    "        forcar_data=colunas_forcar_data,\n",
    "        explicit_prefix=prefix,\n",
    "        limitar_ao_prefixo=True,\n",
    "        auto_prefix_listas=True,\n",
    "        match_normalizado_nas_listas=True,\n",
    "    )\n",
    "\n",
    "    # 5) renomeia/casta ID e mant√©m √∫ltimo por id+funcionalidade\n",
    "    id_col = f\"id{f}\"\n",
    "    if id_col in df_tratado.columns:\n",
    "        df_tratado = df_tratado.withColumn(id_col, F.col(id_col).cast(T.IntegerType()))\n",
    "    elif \"idfuncionalidade\" in df_tratado.columns:\n",
    "        df_tratado = df_tratado.withColumnRenamed(\"idfuncionalidade\", id_col)\n",
    "\n",
    "    if all(k in df_tratado.columns for k in [id_col, \"funcionalidade\", \"referencia_data\"]):\n",
    "        w = Window.partitionBy(id_col, \"funcionalidade\").orderBy(F.col(\"referencia_data\").desc())\n",
    "        df_tratado = df_tratado.withColumn(\"rn\", F.row_number().over(w)).filter(\"rn = 1\").drop(\"rn\")\n",
    "\n",
    "    df_tratado = df_tratado.withColumn(\"data_processamento\",F.from_utc_timestamp(F.current_timestamp(), \"America/Sao_Paulo\"))\n",
    "    print(f\"‚úÖ Tratamento conclu√≠do ({f}) ‚Äî {df_tratado.count()} linhas\")\n",
    "    return df_tratado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d68a9-7716-4d28-b562-b09311c1b02e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def upsert_campos_adicionais_prata(df_campos_tratado, path_campos_prata: str):\n",
    "    # Tabela n√£o existe? cria\n",
    "    if not DeltaTable.isDeltaTable(spark, path_campos_prata):\n",
    "        (df_campos_tratado.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .save(path_campos_prata))\n",
    "        spark.sql(f\"\"\"\n",
    "          ALTER TABLE delta.`{path_campos_prata}`\n",
    "          SET TBLPROPERTIES (\n",
    "            'delta.enableChangeDataFeed'='true',\n",
    "            'delta.autoOptimize.optimizeWrite'='true',\n",
    "            'delta.autoOptimize.autoCompact'='true'\n",
    "          )\n",
    "        \"\"\")\n",
    "        print(f\"üÜï Criada PRATA de Campos Adicionais: {path_campos_prata}\")\n",
    "        return\n",
    "\n",
    "    # Merge incremental\n",
    "    cond = \"t.idfuncionalidade = n.idfuncionalidade AND t.funcionalidade = n.funcionalidade\"\n",
    "    (DeltaTable.forPath(spark, path_campos_prata).alias(\"t\")\n",
    "        .merge(df_campos_tratado.alias(\"n\"), cond)\n",
    "        .whenMatchedUpdateAll(condition=\"n.referencia_data > t.referencia_data\")\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute())\n",
    "\n",
    "    print(\"‚úÖ Merge conclu√≠do em CAMPOS_ADICIONAIS_PRATA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51570b3-34b0-497b-b9be-2c091ef06fff",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def upsert_campos_adicionais_prata(df_campos_tratado, path_campos_prata: str):\n",
    "    # Tabela n√£o existe? cria\n",
    "    if not DeltaTable.isDeltaTable(spark, path_campos_prata):\n",
    "        (df_campos_tratado.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .save(path_campos_prata))\n",
    "        spark.sql(f\"\"\"\n",
    "          ALTER TABLE delta.`{path_campos_prata}`\n",
    "          SET TBLPROPERTIES (\n",
    "            'delta.enableChangeDataFeed'='true',\n",
    "            'delta.autoOptimize.optimizeWrite'='true',\n",
    "            'delta.autoOptimize.autoCompact'='true'\n",
    "          )\n",
    "        \"\"\")\n",
    "        print(f\"üÜï Criada PRATA de Campos Adicionais: {path_campos_prata}\")\n",
    "        return\n",
    "\n",
    "    # Merge incremental\n",
    "    cond = \"t.idfuncionalidade = n.idfuncionalidade AND t.funcionalidade = n.funcionalidade\"\n",
    "    (DeltaTable.forPath(spark, path_campos_prata).alias(\"t\")\n",
    "        .merge(df_campos_tratado.alias(\"n\"), cond)\n",
    "        .whenMatchedUpdateAll(condition=\"n.referencia_data > t.referencia_data\")\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute())\n",
    "\n",
    "    print(\"‚úÖ Merge conclu√≠do em CAMPOS_ADICIONAIS_PRATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5458ecf7-9e9c-4528-9aa0-855ca805d239",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def integrar_campos_no_main_prata(funcionalidade: str, path_main_prata: str, path_campos_prata: str):\n",
    "    \"\"\"Integra os campos adicionais na PRATA principal da funcionalidade.\"\"\"\n",
    "    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "    id_col = f\"id{funcionalidade}\"\n",
    "\n",
    "    if not DeltaTable.isDeltaTable(spark, path_main_prata):\n",
    "        raise ValueError(f\"Tabela principal da PRATA n√£o existe: {path_main_prata}\")\n",
    "\n",
    "    df_check = spark.read.format(\"delta\").load(path_main_prata)\n",
    "    cols_existentes = df_check.columns\n",
    "\n",
    "    # Garante colunas de refer√™ncia no main\n",
    "    if \"referencia_campos_adicionais\" not in cols_existentes or \"referencia_data_campos_adicionais\" not in cols_existentes:\n",
    "        print(f\"ü©π Adicionando colunas de refer√™ncia em {path_main_prata}\")\n",
    "        (\n",
    "            df_check\n",
    "            .withColumn(\"referencia_campos_adicionais\", F.lit(None).cast(T.StringType()))\n",
    "            .withColumn(\"referencia_data_campos_adicionais\", F.lit(None).cast(T.TimestampType()))\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .save(path_main_prata)\n",
    "        )\n",
    "\n",
    "    df_main_ids = spark.read.format(\"delta\").load(path_main_prata).select(id_col).dropDuplicates()\n",
    "    df_campos   = spark.read.format(\"delta\").load(path_campos_prata)\n",
    "\n",
    "    cols_func = [c for c in df_campos.columns if c.startswith(f\"{funcionalidade}_\")] + CHAVES_GRUPO\n",
    "    df_campos_f = (\n",
    "        df_campos\n",
    "          .select(*[c for c in cols_func if c in df_campos.columns])\n",
    "          .filter(F.col(\"funcionalidade\") == funcionalidade)\n",
    "    )\n",
    "\n",
    "    # mant√©m apenas IDs existentes no main\n",
    "    df_campos_f = df_campos_f.join(\n",
    "        df_main_ids.select(F.col(id_col).alias(\"idfuncionalidade\")),\n",
    "        on=\"idfuncionalidade\", how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # renomeia refs\n",
    "    df_campos_f = (\n",
    "        df_campos_f\n",
    "          .withColumnRenamed(\"idfuncionalidade\", \"id_join_tmp\")\n",
    "          .withColumnRenamed(\"referencia\",      \"referencia_campos_adicionais\")\n",
    "          .withColumnRenamed(\"referencia_data\", \"referencia_data_campos_adicionais\")\n",
    "    )\n",
    "\n",
    "    # aplica prefixo \"campos_adicionais_\" √†s colunas de conte√∫do\n",
    "    out_cols = [\n",
    "        F.col(\"id_join_tmp\").alias(id_col),\n",
    "        F.col(\"funcionalidade\"),\n",
    "        F.col(\"referencia_campos_adicionais\"),\n",
    "        F.col(\"referencia_data_campos_adicionais\"),\n",
    "    ]\n",
    "    for c in df_campos_f.columns:\n",
    "        if c not in [\"id_join_tmp\",\"funcionalidade\",\"referencia_campos_adicionais\",\"referencia_data_campos_adicionais\"]:\n",
    "            out_cols.append(F.col(c).alias(f\"campos_adicionais_{c}\"))\n",
    "    df_out = df_campos_f.select(*out_cols)\n",
    "\n",
    "    # MERGE ‚Üí atualiza quando a ref de campos for mais nova\n",
    "    delta_main = DeltaTable.forPath(spark, path_main_prata)\n",
    "    cond = f\"t.{id_col} = n.{id_col}\"\n",
    "    update_cols = [c for c in df_out.columns if c != id_col]\n",
    "    set_expr = {c: f\"n.{c}\" for c in update_cols}\n",
    "\n",
    "    (\n",
    "        delta_main.alias(\"t\")\n",
    "        .merge(df_out.alias(\"n\"), cond)\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"\"\"\n",
    "                n.referencia_data_campos_adicionais >\n",
    "                coalesce(t.referencia_data_campos_adicionais, timestamp'1900-01-01')\n",
    "            \"\"\",\n",
    "            set=set_expr\n",
    "        )\n",
    "        .whenNotMatchedInsert(values=set_expr | {id_col: f\"n.{id_col}\"})\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    print(f\"üîó Vincula√ß√£o aplicada em {path_main_prata} ({len(update_cols)} colunas atualiz√°veis)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc0c3e-8485-4312-a3b7-8b1327d95977",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def processar_uma_funcionalidade(path_bronze: str, path_prata: str, funcionalidade: str):\n",
    "    \"\"\"\n",
    "    L√™ incremental, PIVOTA e TRATA somente a funcionalidade pedida.\n",
    "    N√ÉO escreve automaticamente; retorna o DF tratado.\n",
    "    \"\"\"\n",
    "    df_bronze_inc, _, _ = ler_incremental_bronze_campos(path_bronze, path_prata, funcionalidade)\n",
    "    if df_bronze_inc.rdd.isEmpty():\n",
    "        print(\"‚ÑπÔ∏è Nenhum novo registro na Bronze.\")\n",
    "        return None\n",
    "\n",
    "    df_bronze_inc.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    print(f\"‚úÖ Registros incrementais obtidos: {df_bronze_inc.count()}\")\n",
    "\n",
    "    df_pivot, _ = _pivotar_com_prefixo_debug(df_bronze_inc)\n",
    "    df_func = tratar_funcionalidade(df_pivot, funcionalidade)\n",
    "    return df_func\n",
    "\n",
    "def processar_varias_funcionalidades(path_bronze: str, path_prata: str, funcionalidades: list):\n",
    "    \"\"\"\n",
    "    L√™ incremental uma vez (por funcionalidade), pivota e trata cada uma.\n",
    "    Retorna dict {funcionalidade: df_tratado}\n",
    "    \"\"\"\n",
    "    resultado = {}\n",
    "    for f in funcionalidades:\n",
    "        df = processar_uma_funcionalidade(path_bronze, path_prata, f)\n",
    "        if df is not None:\n",
    "            resultado[f] = df\n",
    "    return resultado\n",
    "\n",
    "def rodar_pipeline_campos_adicionais(funcionalidade: str):\n",
    "    print(f\"\\nüöÄ PIPELINE CAMPOS ADICIONAIS :: {funcionalidade.upper()}\")\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # 1) processa (ler ‚Üí pivotar ‚Üí tratar)\n",
    "    df_trat = processar_uma_funcionalidade(path_campos_bronze, path_campos_prata, funcionalidade)\n",
    "    if df_trat is None:\n",
    "        print(\"‚ÑπÔ∏è Nada para processar.\")\n",
    "        return\n",
    "\n",
    "    # 2) (NOVO) sanitiza ANTES do upsert\n",
    "    df_out = df_trat.select(\n",
    "        F.col(f\"id{funcionalidade}\").alias(\"idfuncionalidade\"),\n",
    "        \"funcionalidade\", \"referencia\", \"referencia_data\",\n",
    "        *[c for c in df_trat.columns if c.startswith(f\"{funcionalidade}_\")]\n",
    "    )\n",
    "    df_out = sanitize_for_sql_endpoint(df_out)  # <- aqui!\n",
    "\n",
    "    # 3) upsert na PRATA (campos adicionais)\n",
    "    upsert_campos_adicionais_prata(df_out, path_campos_prata)\n",
    "\n",
    "    # 4) integrar no main da PRATA\n",
    "    if funcionalidade not in PATHS_MAIN_PRATA:\n",
    "        raise ValueError(f\"PATH principal da PRATA n√£o mapeado para '{funcionalidade}'.\")\n",
    "    integrar_campos_no_main_prata(\n",
    "        funcionalidade=funcionalidade,\n",
    "        path_main_prata=PATHS_MAIN_PRATA[funcionalidade],\n",
    "        path_campos_prata=path_campos_prata\n",
    "    )\n",
    "\n",
    "    print(f\"üèÅ Pipeline conclu√≠do ({funcionalidade}) em {time.perf_counter() - t0:,.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f71a0-f840-4b42-85a4-4ca8dd0c989f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "rodar_pipeline_campos_adicionais(\"reserva\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f08a4-2d04-43a6-b564-01c027ae65bd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "rodar_pipeline_campos_adicionais(\"repasse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22347c30-469b-4223-91be-0be67450015f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "rodar_pipeline_campos_adicionais(\"precadastro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f5db8-57cc-4027-95e3-59a8e85790c0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "rodar_pipeline_campos_adicionais(\"lead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c5b294-1b20-45a1-a680-c46b029c8c71",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## [**Workflow**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ecdd41-46cf-417e-87b1-466af6ce8e82",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def cast_id_columns_to_int(df):\n",
    "    \"\"\"\n",
    "    Converte toda coluna cujo nome come√ßa com 'id' (case-insensitive) para Integer.\n",
    "    - Somente valores estritamente num√©ricos s√£o convertidos.\n",
    "    - N√£o num√©ricos viram NULL (mant√©m consist√™ncia).\n",
    "    - Se j√° for IntegerType, mant√©m.\n",
    "    \"\"\"\n",
    "    out = df\n",
    "    for f in out.schema.fields:\n",
    "        c = f.name\n",
    "        if c.lower().startswith(\"id\") and not isinstance(f.dataType, T.IntegerType):\n",
    "            s = F.col(c).cast(\"string\")\n",
    "            out = out.withColumn(\n",
    "                c,\n",
    "                F.when(F.trim(s).rlike(r\"^[0-9]+$\"), s.cast(T.IntegerType())).otherwise(F.lit(None).cast(T.IntegerType()))\n",
    "            )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc22e129-b0b9-42d4-a7c4-223a1184bfb3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def tratar_workflow_incremental(nome_tabela: str, id_col: str):\n",
    "    \"\"\"\n",
    "    üîß Trata tabelas de workflow (leads, precadastro, reserva, repasse, etc.)\n",
    "    - Ordena por referencia_data (mais recente primeiro)\n",
    "    - Cria colunas de situacao_anterior e situacao_proxima\n",
    "    - Incremental autom√°tico: se existir PRATA, faz merge; sen√£o, cria\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nüöÄ Iniciando tratamento incremental do workflow '{nome_tabela}'\")\n",
    "\n",
    "    # Caminhos\n",
    "    path_bronze = f\"{bronze_lakehouse_path}/{nome_tabela}_workflow_bronze_cvdw\"\n",
    "    path_prata  = f\"{prata_lakehouse_path}/{nome_tabela}_workflow_prata\"\n",
    "\n",
    "    # Verifica se a Bronze existe\n",
    "    if not DeltaTable.isDeltaTable(spark, path_bronze):\n",
    "        print(f\"‚ö†Ô∏è Tabela BRONZE '{nome_tabela}' n√£o encontrada. Pulando.\")\n",
    "        return\n",
    "\n",
    "    # L√™ Bronze\n",
    "    df_bronze = spark.read.format(\"delta\").load(path_bronze)\n",
    "    df_bronze.createOrReplaceTempView(\"bronze\")\n",
    "\n",
    "    # Se PRATA n√£o existe ‚Üí cria do zero\n",
    "    if not DeltaTable.isDeltaTable(spark, path_prata):\n",
    "        print(f\"üÜï Nenhuma PRATA existente. Criando do zero para '{nome_tabela}'.\")\n",
    "\n",
    "        df_final = spark.sql(f\"\"\"\n",
    "            WITH base AS (\n",
    "                SELECT\n",
    "                    *,\n",
    "                    LAG(situacao) OVER (PARTITION BY {id_col} ORDER BY referencia_data DESC)  AS situacao_proxima,\n",
    "                    LEAD(situacao) OVER (PARTITION BY {id_col} ORDER BY referencia_data DESC) AS situacao_anterior\n",
    "                FROM bronze\n",
    "            )\n",
    "            SELECT\n",
    "                referencia,\n",
    "                {id_col},\n",
    "                referencia_data,\n",
    "                situacao,\n",
    "                situacao_anterior,\n",
    "                situacao_proxima,\n",
    "                data_cad,\n",
    "                tempo,\n",
    "                ROUND(tempo / 60, 2)   AS tempo_minutos,\n",
    "                ROUND(tempo / 3600, 2) AS tempo_horas,\n",
    "                ROUND(tempo / 86400, 2) AS tempo_dias,\n",
    "                sistema_origem,\n",
    "                CURRENT_TIMESTAMP() AS data_processamento\n",
    "            FROM base\n",
    "        \"\"\")\n",
    "            # ‚úÖ for√ßa ids como Integer\n",
    "        df_final = cast_id_columns_to_int(df_final)\n",
    "\n",
    "        (\n",
    "            df_final.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "            .save(path_prata)\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ Tabela PRATA criada com sucesso em {path_prata}\")\n",
    "        return\n",
    "\n",
    "    # Caso PRATA exista ‚Üí incremental\n",
    "    print(f\"üîÅ PRATA encontrada ‚Äî realizando merge incremental...\")\n",
    "    df_prata = spark.read.format(\"delta\").load(path_prata)\n",
    "    df_prata.createOrReplaceTempView(\"prata\")\n",
    "\n",
    "    ultima_ref = df_prata.selectExpr(\"max(referencia_data)\").first()[0]\n",
    "    print(f\"üìÖ √öltima referencia_data processada: {ultima_ref}\")\n",
    "\n",
    "    # L√™ apenas incrementais\n",
    "    df_bronze_inc = spark.sql(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM bronze\n",
    "        WHERE referencia_data > TIMESTAMP('{ultima_ref}')\n",
    "    \"\"\")\n",
    "    df_bronze_inc.createOrReplaceTempView(\"bronze_inc\")\n",
    "\n",
    "    if df_bronze_inc.isEmpty():\n",
    "        print(f\"‚ÑπÔ∏è Nenhum novo registro incremental em '{nome_tabela}'.\")\n",
    "        return\n",
    "\n",
    "    # Processa os incrementais com SQL\n",
    "    df_tratado = spark.sql(f\"\"\"\n",
    "        WITH base AS (\n",
    "            SELECT\n",
    "                *,\n",
    "                LAG(situacao) OVER (PARTITION BY {id_col} ORDER BY referencia_data DESC)  AS situacao_proxima,\n",
    "                LEAD(situacao) OVER (PARTITION BY {id_col} ORDER BY referencia_data DESC) AS situacao_anterior\n",
    "            FROM bronze_inc\n",
    "        )\n",
    "        SELECT\n",
    "            referencia,\n",
    "            {id_col},\n",
    "            referencia_data,\n",
    "            situacao,\n",
    "            situacao_anterior,\n",
    "            situacao_proxima,\n",
    "            data_cad,\n",
    "            tempo,\n",
    "            ROUND(tempo / 60, 2)   AS tempo_minutos,\n",
    "            ROUND(tempo / 3600, 2) AS tempo_horas,\n",
    "            ROUND(tempo / 86400, 2) AS tempo_dias,\n",
    "            sistema_origem,\n",
    "            CURRENT_TIMESTAMP() AS data_processamento\n",
    "        FROM base\n",
    "    \"\"\")\n",
    "    df_tratado = cast_id_columns_to_int(df_tratado)\n",
    "    # Faz o merge incremental no Delta\n",
    "    delta = DeltaTable.forPath(spark, path_prata)\n",
    "    (\n",
    "        delta.alias(\"t\")\n",
    "        .merge(df_tratado.alias(\"n\"), \"t.referencia = n.referencia\")\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Incremento conclu√≠do com sucesso em '{nome_tabela}_workflow_prata'.\")\n",
    "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b2c6f1-3744-4dc4-9bf2-b245c1806e8e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "tratar_workflow_incremental(\"precadastros\", \"idprecadastro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f0ab0-29f0-4b81-98b3-2605b3f18dc6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "tratar_workflow_incremental(\"leads\", \"idlead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c396e1e-b054-41ed-acd3-dcb12de23869",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "tratar_workflow_incremental(\"repasse\", \"idrepasse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3859f05-29fa-4cff-8a80-bede84a0e08b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "tratar_workflow_incremental(\"reservas\", \"idreserva\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda28a1-c1dd-495d-98c8-dc0367ac8958",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## [**Tratamento Tabelas Dimens√µes**]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be22d1e-d229-4a72-bc3a-bcff34f942d6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### [**Unidades**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf391d1-b335-497c-851b-4446fb9bd874",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ‚öôÔ∏è CAMINHOS F√çSICOS DAS TABELAS BRONZE\n",
    "# ================================================================\n",
    "path_unidades = f\"{bronze_lakehouse_path}/unidades_bronze_cvdw\"\n",
    "# ================================================================\n",
    "# ‚öôÔ∏è CAMINHOS F√çSICOS DAS TABELAS PRATA\n",
    "# ================================================================\n",
    "path_unidades_prata  = f\"{prata_lakehouse_path}/unidades_prata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f2f20-35ca-4bd3-8152-48a45d164642",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- UNIDADES -------------------\n",
    "df_unidades_bronze = ler_incremental_bronze(\n",
    "    nome_tabela=\"unidades\",\n",
    "    path_bronze=path_unidades,\n",
    "    path_prata=path_unidades_prata,\n",
    "    id_col=\"idunidade\"\n",
    ")\n",
    "if df_unidades_bronze is not None and not df_unidades_bronze.rdd.isEmpty():\n",
    "    print(f\"‚úÖ df_unidades_bronze criado com sucesso ‚Äî {df_unidades_bronze.count()} registros incrementais.\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado incremental carregado para 'unidades'.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8f59d-5716-488a-a5dc-bedb9228ad6e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df_unidades_bronze       = normalizar_nomes_colunas(df_unidades_bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727cb479-3b26-4230-b248-c707f5234c55",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# exclus√µes usando nomes *normalizados* (a fun√ß√£o resolve para o nome real)\n",
    "excluir_valor = []\n",
    "excluir_data  = [\"situacao_reservada_vencimento\"]\n",
    "\n",
    "df_unidades_bronze = tratar_valores_e_datas(\n",
    "    df_unidades_bronze,\n",
    "    excluir_valor=excluir_valor,\n",
    "    excluir_data=excluir_data,\n",
    "    explicit_prefix=None,          # sem prefixo\n",
    "    limitar_ao_prefixo=False,      # varre todas as colunas\n",
    "    auto_prefix_listas=False,      # n√£o tenta prefixar listas\n",
    "    match_normalizado_nas_listas=True  # casa nomes normalizados com os reais\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RESERVA tratados e tipados com sucesso.\")\n",
    "df_unidades_bronze.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176ca11-e12c-472f-ac75-360125f8c158",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "verificar_ids_com_virgula(df_unidades_bronze, \"UNIDADES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a1c9d-5c54-4cca-8da4-12e698179b0a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Ex.: UNIDADES\n",
    "df_unidade_prata = preparar_para_prata(df_unidades_bronze, coluna_id=\"idunidade\", sistema_origem=\"CVCRM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a0095a-107e-4d61-acde-2f5851eea2ce",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "print(\"üè∑Ô∏è Gerando coluna 'sigla_empreendimento' com base no nome...\")\n",
    "\n",
    "df_unidade_prata = (\n",
    "    df_unidade_prata\n",
    "    .withColumn(\n",
    "        \"sigla_empreendimento\",\n",
    "        F.when(F.upper(F.col(\"nome_empreendimento\")).like(\"%FSA%\"), \"FSA\")\n",
    "         .when(F.upper(F.col(\"nome_empreendimento\")).like(\"%AGL%\"), \"AGL\")\n",
    "         .when(F.upper(F.col(\"nome_empreendimento\")).like(\"%DF%\"),  \"DF\")\n",
    "         .when(F.upper(F.col(\"nome_empreendimento\")).like(\"%OCD%\"), \"OCD\")\n",
    "         .when(F.upper(F.col(\"nome_empreendimento\")).like(\"%GOI%\"), \"GOI\")\n",
    "         .when(F.upper(F.col(\"nome_empreendimento\")).like(\"%GOY%\"), \"GOY\")\n",
    "         .when(F.upper(F.col(\"nome_empreendimento\")).like(\"%RETOM%\"), \"RETOMADO\")\n",
    "         .otherwise(\"OUTROS\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Coluna 'sigla_empreendimento' criada com sucesso a partir do nome.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e3bc66-3d7d-4edb-b2cd-2268d339b045",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "print(\"üè∑Ô∏è Gerando coluna 'nome_empreendimento_reduzido' com base no nome...\")\n",
    "\n",
    "df_unidade_prata = (\n",
    "    df_unidade_prata\n",
    "    .withColumn(\n",
    "        \"nome_empreendimento_reduzido\",\n",
    "        F.trim(F.split(F.col(\"nome_empreendimento\"), \"-\").getItem(0))\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Coluna 'sigla_empreendimento' criada com sucesso a partir do nome.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75caa95c-eaaa-4be1-b141-31d4ed280ebc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "upsert_prata(df_unidade_prata, f\"{prata_lakehouse_path}/unidade_prata\", coluna_id=\"idunidade\", zorder_cols=[\"idunidade\",\"referencia_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a649e5-e6d4-4046-b023-4d00c78070a0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### [**Vendas**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95631f9-4879-4eee-baed-456ac8efd3ae",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ‚öôÔ∏è CAMINHOS F√çSICOS DAS TABELAS BRONZE\n",
    "# ================================================================\n",
    "path_vendas = f\"{bronze_lakehouse_path}/vendas_bronze_cvdw\"\n",
    "# ================================================================\n",
    "# ‚öôÔ∏è CAMINHOS F√çSICOS DAS TABELAS PRATA\n",
    "# ================================================================\n",
    "path_vendas_prata  = f\"{prata_lakehouse_path}/vendas_prata\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a9fcc5-e99b-4dfe-b01d-96c38c960624",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- VENDAS -------------------\n",
    "df_vendas_bronze = ler_incremental_bronze(\n",
    "    nome_tabela=\"vendas_bronze_cvdw\",\n",
    "    path_bronze=path_vendas,\n",
    "    path_prata=path_vendas_prata,\n",
    "    id_col=\"referencia\"\n",
    ")\n",
    "if df_vendas_bronze is not None and not df_vendas_bronze.rdd.isEmpty():\n",
    "    print(f\"‚úÖ df_vendas_bronze criado com sucesso ‚Äî {df_vendas_bronze.count()} registros incrementais.\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado incremental carregado para 'vendas'.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60beeb2-4ca9-49b5-a978-51ca2e1200df",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df_vendas_bronze       = normalizar_nomes_colunas(df_vendas_bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d4e26f-a0b4-48c8-8cba-a3b0aacb2eba",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# exclus√µes usando nomes *normalizados* (a fun√ß√£o resolve para o nome real)\n",
    "excluir_valor = []\n",
    "excluir_data  = []\n",
    "\n",
    "df_vendas_bronze = tratar_valores_e_datas(\n",
    "    df_vendas_bronze,\n",
    "    excluir_valor=excluir_valor,\n",
    "    excluir_data=excluir_data,\n",
    "    explicit_prefix=None,          # sem prefixo\n",
    "    limitar_ao_prefixo=False,      # varre todas as colunas\n",
    "    auto_prefix_listas=False,      # n√£o tenta prefixar listas\n",
    "    match_normalizado_nas_listas=True  # casa nomes normalizados com os reais\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vendas tratados e tipados com sucesso.\")\n",
    "df_vendas_bronze.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03200784-afa9-44a6-aabf-d48322377e03",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "verificar_ids_com_virgula(df_vendas_bronze, \"VENDAS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bfff55-fe6a-475f-9804-570e296793fd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Ex.: VENDAS\n",
    "df_vendas_prata = preparar_para_prata(df_vendas_bronze, coluna_id=\"referencia\", sistema_origem=\"CVCRM\")\n",
    "df_vendas_prata.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182f2ba-d5f8-46ce-b5bd-5552ed455135",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "upsert_prata(df_vendas_prata, f\"{prata_lakehouse_path}/venda_prata\", coluna_id=\"referencia\", zorder_cols=[\"referencia\",\"referencia_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f873151-0672-42b5-a9b9-8cd31207afef",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### [**Reserva Registro Flags**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcf326-f3cc-417c-a79c-81b426e7e03e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ‚öôÔ∏è CAMINHOS F√çSICOS DAS TABELAS BRONZE\n",
    "# ================================================================\n",
    "path_reservas_registros_flags = f\"{bronze_lakehouse_path}/reservas_registros_flags_bronze_cvdw\"\n",
    "\n",
    "# ================================================================\n",
    "# ‚öôÔ∏è CAMINHOS F√çSICOS DAS TABELAS PRATA\n",
    "# ================================================================\n",
    "path_reservas_registros_flags_prata  = f\"{prata_lakehouse_path}/reservas_registros_flags_prata\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961738c7-fa7f-44a4-ad2e-0005bfa031e9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- Reservas Registros Flags -------------------\n",
    "df_reservas_registros_flags_bronze = ler_incremental_bronze(\n",
    "    nome_tabela=\"reservas_registros_flags_bronze_cvdw\",\n",
    "    path_bronze=path_reservas_registros_flags,\n",
    "    path_prata=path_reservas_registros_flags_prata,\n",
    "    id_col=\"referencia\"\n",
    ")\n",
    "if df_reservas_registros_flags_bronze is not None and not df_reservas_registros_flags_bronze.rdd.isEmpty():\n",
    "    print(f\"‚úÖ df_vendas_bronze criado com sucesso ‚Äî {df_reservas_registros_flags_bronze.count()} registros incrementais.\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado incremental carregado para 'vendas'.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e526b-b709-47de-a228-30f7a83ea6b5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df_reservas_registros_flags_bronze       = normalizar_nomes_colunas(df_reservas_registros_flags_bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429313f-40cb-40bb-b1a4-e56b21fd9fde",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# exclus√µes usando nomes *normalizados* (a fun√ß√£o resolve para o nome real)\n",
    "excluir_valor = []\n",
    "excluir_data  = []\n",
    "\n",
    "df_reservas_registros_flags_bronze = tratar_valores_e_datas(\n",
    "    df_reservas_registros_flags_bronze,\n",
    "    excluir_valor=excluir_valor,\n",
    "    excluir_data=excluir_data,\n",
    "    explicit_prefix=None,          # sem prefixo\n",
    "    limitar_ao_prefixo=False,      # varre todas as colunas\n",
    "    auto_prefix_listas=False,      # n√£o tenta prefixar listas\n",
    "    match_normalizado_nas_listas=True  # casa nomes normalizados com os reais\n",
    ")\n",
    "\n",
    "print(\"‚úÖ df_reservas_registros_flags_bronze tratados e tipados com sucesso.\")\n",
    "df_reservas_registros_flags_bronze.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c27a05-0246-4739-958a-1c51d28a18ed",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "display(df_reservas_registros_flags_bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e26d64e-d523-46bb-a2ab-5720fb78954e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "upsert_prata(df_reservas_registros_flags_bronze, f\"{prata_lakehouse_path}/reservas_registros_flags_prata\", coluna_id=\"idreserva\", zorder_cols=[\"idreserva\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822312d-b6e8-4fed-a382-eb7fb12978c6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### [**Corretores**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b83ce69-b52b-4356-9edd-1d967f0eb689",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ‚öôÔ∏è CAMINHOS F√çSICOS DAS TABELAS BRONZE\n",
    "# ================================================================\n",
    "path_corretores = f\"{bronze_lakehouse_path}/corretores_bronze_cvdw\"\n",
    "\n",
    "# ================================================================\n",
    "# ‚öôÔ∏è CAMINHOS F√çSICOS DAS TABELAS PRATA\n",
    "# ================================================================\n",
    "path_corretores_prata  = f\"{prata_lakehouse_path}/corretores_prata\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3043c-7e08-4161-9f0c-2dab7e9aff1b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- Corretores -------------------\n",
    "df_corretores_bronze = ler_incremental_bronze(\n",
    "    nome_tabela=\"corretores\",\n",
    "    path_bronze=path_corretores,\n",
    "    path_prata=path_corretores_prata,\n",
    "    id_col=\"referencia\"\n",
    ")\n",
    "if df_corretores_bronze is not None and not df_corretores_bronze.rdd.isEmpty():\n",
    "    print(f\"‚úÖ df_corretores_bronze criado com sucesso ‚Äî {df_corretores_bronze.count()} registros incrementais.\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado incremental carregado para 'vendas'.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73a8800-f9e6-4f10-9120-0cc86efb06ef",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df_corretores_bronze       = normalizar_nomes_colunas(df_corretores_bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a381f5-7297-46d1-a391-a82c1b96056c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# exclus√µes usando nomes *normalizados* (a fun√ß√£o resolve para o nome real)\n",
    "excluir_valor = []\n",
    "excluir_data  = []\n",
    "\n",
    "df_corretores_bronze = tratar_valores_e_datas(\n",
    "    df_corretores_bronze,\n",
    "    excluir_valor=excluir_valor,\n",
    "    excluir_data=excluir_data,\n",
    "    explicit_prefix=None,          # sem prefixo\n",
    "    limitar_ao_prefixo=False,      # varre todas as colunas\n",
    "    auto_prefix_listas=False,      # n√£o tenta prefixar listas\n",
    "    match_normalizado_nas_listas=True  # casa nomes normalizados com os reais\n",
    ")\n",
    "\n",
    "print(\"‚úÖ df_corretores_bronze tratados e tipados com sucesso.\")\n",
    "df_corretores_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c0daa-ec53-4daa-b2d6-8ec4148335bb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "upsert_prata(df_corretores_bronze, f\"{prata_lakehouse_path}/corretores_prata\", coluna_id=\"referencia\", zorder_cols=[\"referencia\",\"referencia_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b43b35-65dc-47cc-8be3-555cf660e78a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### **Imobiliarias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5795ce9-e850-4214-9834-f61f588c143c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ‚öôÔ∏è CAMINHOS F√çSICOS DAS TABELAS BRONZE\n",
    "# ================================================================\n",
    "path_imobiliarias = f\"{bronze_lakehouse_path}/imobiliarias_bronze_cvdw\"\n",
    "\n",
    "# ================================================================\n",
    "# ‚öôÔ∏è CAMINHOS F√çSICOS DAS TABELAS PRATA\n",
    "# ================================================================\n",
    "path_imobiliarias_prata  = f\"{prata_lakehouse_path}/imobiliarias_prata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4944b8ef-527b-4fb9-9625-c97152fc0e68",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- Corretores -------------------\n",
    "df_imobiliarias_bronze = ler_incremental_bronze(\n",
    "    nome_tabela=\"imobiliarias\",\n",
    "    path_bronze=path_imobiliarias,\n",
    "    path_prata=path_imobiliarias_prata,\n",
    "    id_col=\"referencia\"\n",
    ")\n",
    "if df_imobiliarias_bronze is not None and not df_imobiliarias_bronze.rdd.isEmpty():\n",
    "    print(f\"‚úÖ df_imobiliarias_bronze criado com sucesso ‚Äî {df_imobiliarias_bronze.count()} registros incrementais.\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado incremental carregado para 'Imobiliaria'.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fb7d7-7864-4886-bf4a-2dea62ec7849",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df_imobiliarias_bronze       = normalizar_nomes_colunas(df_imobiliarias_bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c08a693-0a14-457b-9135-9a1fa84a7e3a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# exclus√µes usando nomes *normalizados* (a fun√ß√£o resolve para o nome real)\n",
    "excluir_valor = []\n",
    "excluir_data  = []\n",
    "\n",
    "df_imobiliarias_bronze = tratar_valores_e_datas(\n",
    "    df_imobiliarias_bronze,\n",
    "    excluir_valor=excluir_valor,\n",
    "    excluir_data=excluir_data,\n",
    "    explicit_prefix=None,          # sem prefixo\n",
    "    limitar_ao_prefixo=False,      # varre todas as colunas\n",
    "    auto_prefix_listas=False,      # n√£o tenta prefixar listas\n",
    "    match_normalizado_nas_listas=True  # casa nomes normalizados com os reais\n",
    ")\n",
    "\n",
    "print(\"‚úÖ df_corretores_bronze tratados e tipados com sucesso.\")\n",
    "df_imobiliarias_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dee6a2-c867-45f8-bb58-0a1eaf46beaa",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "upsert_prata(df_imobiliarias_bronze, f\"{prata_lakehouse_path}/imobiliarias_prata\", coluna_id=\"referencia\", zorder_cols=[\"referencia\",\"referencia_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60d4b9a-d492-4c32-94d7-af3526cb3323",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### [**Distrato**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6446615a-ef72-4e8a-834c-1d652b74d950",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ‚öôÔ∏è CAMINHOS F√çSICOS DAS TABELAS BRONZE\n",
    "# ================================================================\n",
    "path_distratos = f\"{bronze_lakehouse_path}/distratos_cvdw\"\n",
    "\n",
    "# ================================================================\n",
    "# ‚öôÔ∏è CAMINHOS F√çSICOS DAS TABELAS PRATA\n",
    "# ================================================================\n",
    "path_distratos_prata  = f\"{prata_lakehouse_path}/distratos_prata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d141bbd-4703-423b-be32-e13980181b37",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------- Corretores -------------------\n",
    "df_distratos_bronze = ler_incremental_bronze(\n",
    "    nome_tabela=\"distratos\",\n",
    "    path_bronze=path_distratos,\n",
    "    path_prata=path_distratos_prata,\n",
    "    id_col=\"referencia\"\n",
    ")\n",
    "if df_distratos_bronze is not None and not df_distratos_bronze.rdd.isEmpty():\n",
    "    print(f\"‚úÖ df_distratos_bronze criado com sucesso ‚Äî {df_distratos_bronze.count()} registros incrementais.\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado incremental carregado para 'Imobiliaria'.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c40fb7-49f5-4b87-a186-82aeb00aae72",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df_distratos_bronze       = normalizar_nomes_colunas(df_distratos_bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f80fe-e82b-4069-bdae-80c082223a1d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# exclus√µes usando nomes *normalizados* (a fun√ß√£o resolve para o nome real)\n",
    "excluir_valor = []\n",
    "excluir_data  = []\n",
    "\n",
    "df_distratos_bronze = tratar_valores_e_datas(\n",
    "    df_distratos_bronze,\n",
    "    excluir_valor=excluir_valor,\n",
    "    excluir_data=excluir_data,\n",
    "    explicit_prefix=None,          # sem prefixo\n",
    "    limitar_ao_prefixo=False,      # varre todas as colunas\n",
    "    auto_prefix_listas=False,      # n√£o tenta prefixar listas\n",
    "    match_normalizado_nas_listas=True  # casa nomes normalizados com os reais\n",
    ")\n",
    "\n",
    "print(\"‚úÖ df_corretores_bronze tratados e tipados com sucesso.\")\n",
    "df_distratos_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f076c-a8d6-4850-8cd0-7238e3e7d572",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "upsert_prata(df_distratos_bronze, f\"{prata_lakehouse_path}/distratos_prata\", coluna_id=\"referencia\", zorder_cols=[\"referencia\",\"referencia_data\"])"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "23eaa952-7390-4f45-ae77-4a670b5fb276",
    "default_lakehouse_name": "PRATA_7LM",
    "default_lakehouse_workspace_id": "5306a0f7-da2d-4af0-83b0-30034e986e84",
    "known_lakehouses": [
     {
      "id": "23eaa952-7390-4f45-ae77-4a670b5fb276"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "2400000"
    }
   }
  },
  "synapse_widget": {
   "state": {
    "f0430cb9-6cfd-43eb-a065-e2897a3739ba": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "sum",
        "binsNumber": 10,
        "categoryFieldKeys": [],
        "chartType": "bar",
        "isStacked": false,
        "seriesFieldKeys": [],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "scala",
      "table": {
       "rows": [
        {
         "0": "5288",
         "1": "2025-12-01 12:04:03",
         "2": "S",
         "3": "5288",
         "4": "2025-03-10 18:43:03",
         "5": "DF 01 - Haus By Novka",
         "6": "21",
         "7": "ETAPA 01",
         "8": "25",
         "9": "TORRE",
         "10": "148",
         "11": "202",
         "12": "2611",
         "13": "GABRIEL SARKIS MUNDIM",
         "14": "23358",
         "15": "Psi Negocios Imobiliarios LTDA",
         "16": "277",
         "17": "Sim",
         "18": "2025-03-10 18:43:03",
         "23": "Sim",
         "24": "2025-03-24 17:21:19",
         "39": "Sim",
         "40": "2025-03-24 13:16:09",
         "56": "2025-03-24 12:05:46",
         "63": "2025-12-01 12:05:59.496109",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 0,
         "key": 0
        },
        {
         "0": "5929",
         "1": "2025-12-01 10:44:44",
         "2": "S",
         "3": "5929",
         "4": "2025-08-18 16:56:13",
         "5": "AGL30 - VILA GIRASSOL",
         "6": "27",
         "7": "Etapa Unica",
         "8": "62",
         "9": "BLOCO E",
         "10": "394",
         "11": "104",
         "12": "9494",
         "13": "UILHAN OLIVEIRA EVANGELISTA",
         "14": "21284",
         "15": "Igor da Silva Neves",
         "16": "326",
         "17": "Sim",
         "18": "2025-08-18 16:56:13",
         "23": "Sim",
         "24": "2025-08-20 08:11:14",
         "25": "Sim",
         "26": "2025-12-01 10:44:44",
         "39": "Sim",
         "40": "2025-08-20 07:50:42",
         "56": "2025-08-19 19:51:39",
         "63": "2025-12-01 12:05:59.496109",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 1,
         "key": 1
        },
        {
         "0": "6315",
         "1": "2025-12-01 11:37:26",
         "2": "S",
         "3": "6315",
         "4": "2025-10-29 13:31:46",
         "5": "AGL32 - VILA MARGARIDA",
         "6": "31",
         "7": "Etapa Unica",
         "8": "69",
         "9": "BLOCO D",
         "10": "447",
         "11": "01",
         "12": "10703",
         "13": "MARIA THAUANY ALVES DOMINGOS",
         "14": "28793",
         "15": "Sandra Santos - CLT",
         "16": "379",
         "17": "Sim",
         "18": "2025-11-20 13:24:00",
         "63": "2025-12-01 12:05:59.496109",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 2,
         "key": 2
        },
        {
         "0": "6331",
         "1": "2025-12-01 10:33:11",
         "2": "S",
         "3": "6331",
         "4": "2025-11-04 17:38:26",
         "5": "AGL32 - VILA MARGARIDA",
         "6": "31",
         "7": "Etapa Unica",
         "8": "69",
         "9": "BLOCO B",
         "10": "445",
         "11": "304",
         "12": "10686",
         "13": "MAYRLUCE SOBREIRA DE JESUS",
         "14": "28794",
         "15": "Daniel Andrade dos Santos - CLT",
         "16": "373",
         "17": "Sim",
         "18": "2025-11-04 17:38:26",
         "37": "Sim",
         "38": "2025-12-01 10:33:11",
         "63": "2025-12-01 12:05:59.496109",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 3,
         "key": 3
        },
        {
         "0": "6362",
         "1": "2025-12-01 10:15:16",
         "2": "S",
         "3": "6362",
         "4": "2025-11-13 19:30:18",
         "5": "FSA013 - VILA L√ìTUS",
         "6": "33",
         "7": "Etapa √önica",
         "8": "72",
         "9": "BLOCO 04",
         "10": "462",
         "11": "01",
         "12": "10877",
         "13": "LAYSSA CHRISTINNY DE OLIVEIRA REIS",
         "14": "29104",
         "15": "Ryan Gomes Calazans - CLT",
         "16": "287",
         "17": "Sim",
         "18": "2025-11-25 17:39:54",
         "39": "Sim",
         "40": "2025-12-01 09:32:27",
         "56": "2025-12-01 08:53:17",
         "63": "2025-12-01 12:05:59.496109",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 4,
         "key": 4
        },
        {
         "0": "6385",
         "1": "2025-12-01 10:15:18",
         "2": "S",
         "3": "6385",
         "4": "2025-11-18 15:45:38",
         "5": "AGL30 - VILA GIRASSOL",
         "6": "27",
         "7": "Etapa Unica",
         "8": "62",
         "9": "BLOCO H",
         "10": "397",
         "11": "4",
         "12": "9553",
         "13": "ANDREIA DA CONCEI√áAO",
         "14": "29151",
         "15": "Luciano Pontes Silva Junior - CLT",
         "16": "433",
         "17": "Sim",
         "18": "2025-11-28 09:13:00",
         "39": "Sim",
         "40": "2025-12-01 08:03:33",
         "56": "2025-11-30 12:43:19",
         "63": "2025-12-01 12:05:59.496109",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 5,
         "key": 5
        },
        {
         "0": "6386",
         "1": "2025-12-01 10:15:19",
         "2": "S",
         "3": "6386",
         "4": "2025-11-18 17:45:42",
         "5": "FSA012 - VILA DOS L√çRIOS",
         "6": "30",
         "7": "Etapa Unica",
         "8": "68",
         "9": "BLOCO E",
         "10": "439",
         "11": "102",
         "12": "10580",
         "13": "BRENDA DUTRA TEIXEIRA",
         "14": "29396",
         "15": "Ana Carolina Ferreira de Andrade - Carlos Andrade Imobili√°ria",
         "16": "47",
         "17": "Sim",
         "18": "2025-11-26 14:44:39",
         "39": "Sim",
         "40": "2025-12-01 08:41:37",
         "56": "2025-12-01 08:41:21",
         "63": "2025-12-01 12:05:59.496109",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 6,
         "key": 6
        },
        {
         "0": "6393",
         "1": "2025-12-01 11:37:34",
         "2": "S",
         "3": "6393",
         "4": "2025-11-18 20:13:51",
         "5": "AGL32 - VILA MARGARIDA",
         "6": "31",
         "7": "Etapa Unica",
         "8": "69",
         "9": "BLOCO B",
         "10": "445",
         "11": "103",
         "12": "10677",
         "13": "GUSTAVO SOUZA MAIA",
         "14": "29225",
         "15": "Wanderson Romero Coelho Da Silva",
         "16": "364",
         "17": "Sim",
         "18": "2025-11-25 16:35:56",
         "63": "2025-12-01 12:05:59.496109",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 7,
         "key": 7
        },
        {
         "0": "6397",
         "1": "2025-12-01 11:41:50",
         "2": "S",
         "3": "6397",
         "4": "2025-11-19 14:26:51",
         "5": "AGL30 - VILA GIRASSOL",
         "6": "27",
         "7": "Etapa Unica",
         "8": "62",
         "9": "BLOCO E",
         "10": "394",
         "11": "101",
         "12": "9491",
         "13": "RAFAELA DE SOUZA LIMA",
         "14": "29264",
         "15": "Luciano Pontes Silva Junior - CLT",
         "16": "433",
         "17": "Sim",
         "18": "2025-11-26 18:25:11",
         "39": "Sim",
         "40": "2025-12-01 08:03:24",
         "56": "2025-11-30 12:36:11",
         "63": "2025-12-01 12:05:59.496109",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 8,
         "key": 8
        },
        {
         "0": "6405",
         "1": "2025-12-01 11:40:16",
         "2": "S",
         "3": "6405",
         "4": "2025-11-19 18:50:31",
         "5": "FSA011 - VILA JASMIM",
         "6": "28",
         "7": "Etapa Unica",
         "8": "64",
         "9": "BLOCO E",
         "10": "425",
         "11": "04",
         "12": "9989",
         "13": "CRISTIANO SOUSA GOMES",
         "14": "5208",
         "15": "IMOBILIARIA GOL IM√ìVEIS",
         "16": "422",
         "17": "Sim",
         "18": "2025-11-19 18:50:32",
         "39": "Sim",
         "40": "2025-12-01 08:03:13",
         "56": "2025-11-30 12:35:01",
         "63": "2025-12-01 12:05:59.496109",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 9,
         "key": 9
        },
        {
         "0": "6411",
         "1": "2025-12-01 12:02:28",
         "2": "S",
         "3": "6411",
         "4": "2025-11-22 12:34:44",
         "5": "AGL30 - VILA GIRASSOL",
         "6": "27",
         "7": "Etapa Unica",
         "8": "62",
         "9": "BLOCO K",
         "10": "400",
         "11": "204",
         "12": "9595",
         "13": "LETICIA DAMIANA SOUZA MOURA",
         "14": "29068",
         "15": "IMOBILIARIA EXPERT IMOVEIS",
         "16": "460",
         "17": "Sim",
         "18": "2025-11-28 09:10:25",
         "63": "2025-12-01 12:05:59.496109",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 10,
         "key": 10
        },
        {
         "0": "6438",
         "1": "2025-12-01 10:32:42",
         "2": "S",
         "3": "6438",
         "4": "2025-12-01 10:30:25",
         "5": "FSA013 - VILA L√ìTUS",
         "6": "33",
         "7": "Etapa √önica",
         "8": "72",
         "9": "BLOCO 13",
         "10": "471",
         "11": "203",
         "12": "11031",
         "13": "EDUARDA MELO GARCIA PALACIO",
         "14": "27001",
         "15": "Sandra Santos - CLT",
         "16": "379",
         "17": "Sim",
         "18": "2025-12-01 10:30:25",
         "63": "2025-12-01 12:05:59.496109",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 11,
         "key": 11
        },
        {
         "0": "6389",
         "1": "2025-12-01 08:55:03",
         "2": "S",
         "3": "6389",
         "4": "2025-11-18 18:20:18",
         "5": "FSA012 - VILA DOS L√çRIOS",
         "6": "30",
         "7": "Etapa Unica",
         "8": "68",
         "9": "BLOCO F",
         "10": "440",
         "11": "04",
         "12": "10594",
         "13": "GABRIEL IKENO DUTRA LIMA",
         "14": "29398",
         "15": "Ana Carolina Ferreira de Andrade - Carlos Andrade Imobili√°ria",
         "16": "47",
         "17": "Sim",
         "18": "2025-11-18 18:20:18",
         "63": "2025-12-01 09:17:41.247589",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 12,
         "key": 12
        },
        {
         "0": "6399",
         "1": "2025-12-01 08:54:24",
         "2": "S",
         "3": "6399",
         "4": "2025-11-19 16:22:41",
         "5": "FSA012 - VILA DOS L√çRIOS",
         "6": "30",
         "7": "Etapa Unica",
         "8": "68",
         "9": "BLOCO B",
         "10": "436",
         "11": "203",
         "12": "10537",
         "13": "MAYARA DE SOUSA BRITO",
         "14": "29276",
         "15": "Vit√≥ria Im√≥veis IMOB",
         "16": "89",
         "17": "Sim",
         "18": "2025-11-19 16:22:41",
         "63": "2025-12-01 09:17:41.247589",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 13,
         "key": 13
        },
        {
         "0": "6401",
         "1": "2025-12-01 08:51:42",
         "2": "S",
         "3": "6401",
         "4": "2025-11-19 17:55:43",
         "5": "FSA013 - VILA L√ìTUS",
         "6": "33",
         "7": "Etapa √önica",
         "8": "72",
         "9": "BLOCO 02",
         "10": "460",
         "11": "203",
         "12": "10855",
         "13": "JENIFER ASSIS PAULA",
         "14": "29348",
         "15": "Vit√≥ria Im√≥veis IMOB",
         "16": "89",
         "17": "Sim",
         "18": "2025-11-19 17:55:43",
         "63": "2025-12-01 09:17:41.247589",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 14,
         "key": 14
        },
        {
         "0": "6423",
         "1": "2025-12-01 09:17:02",
         "2": "S",
         "3": "6423",
         "4": "2025-11-27 13:53:53",
         "5": "FSA013 - VILA L√ìTUS",
         "6": "33",
         "7": "Etapa √önica",
         "8": "72",
         "9": "BLOCO 17",
         "10": "475",
         "11": "204",
         "12": "11096",
         "13": "ALEXANDRA ROBERTA PORTO SILVA",
         "14": "26803",
         "15": "ANNA LUIZA PEREIRA DE SOUSA",
         "16": "417",
         "17": "Sim",
         "18": "2025-11-27 13:53:53",
         "63": "2025-12-01 09:17:41.247589",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 15,
         "key": 15
        },
        {
         "0": "6424",
         "1": "2025-12-01 09:16:36",
         "2": "S",
         "3": "6424",
         "4": "2025-11-27 14:58:38",
         "5": "FSA010 - VILA JARDINS",
         "6": "26",
         "7": "Etapa Unica",
         "8": "61",
         "9": "BLOCO D",
         "10": "359",
         "11": "APTO 201",
         "12": "8994",
         "13": "MARIANY OLIVEIRA BORGES",
         "14": "19943",
         "15": "CLEIDE ALVES DE OLIVEIRA",
         "16": "414",
         "17": "Sim",
         "18": "2025-11-27 14:58:38",
         "63": "2025-12-01 09:17:41.247589",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 16,
         "key": 16
        },
        {
         "0": "6426",
         "1": "2025-12-01 09:16:05",
         "2": "S",
         "3": "6426",
         "4": "2025-11-27 18:12:56",
         "5": "FSA010 - VILA JARDINS",
         "6": "26",
         "7": "Etapa Unica",
         "8": "61",
         "9": "BLOCO C",
         "10": "358",
         "11": "APTO 303",
         "12": "8985",
         "13": "WILIAN PEREIRA DE JESUS",
         "14": "29520",
         "15": "ANNA LUIZA PEREIRA DE SOUSA",
         "16": "417",
         "17": "Sim",
         "18": "2025-11-27 18:12:56",
         "63": "2025-12-01 09:17:41.247589",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 17,
         "key": 17
        },
        {
         "0": "6427",
         "1": "2025-12-01 09:15:30",
         "2": "S",
         "3": "6427",
         "4": "2025-11-28 14:09:53",
         "5": "FSA011 - VILA JASMIM",
         "6": "28",
         "7": "Etapa Unica",
         "8": "64",
         "9": "BLOCO H",
         "10": "428",
         "11": "03",
         "12": "10036",
         "13": "LETICIA LOPES RODRIGUES",
         "14": "29553",
         "15": "NAUANY MARQUES DA CONCEI√á√ÉO",
         "16": "468",
         "17": "Sim",
         "18": "2025-11-28 14:09:53",
         "63": "2025-12-01 09:17:41.247589",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 18,
         "key": 18
        },
        {
         "0": "6429",
         "1": "2025-12-01 09:14:58",
         "2": "S",
         "3": "6429",
         "4": "2025-11-28 17:33:49",
         "5": "FSA013 - VILA L√ìTUS",
         "6": "33",
         "7": "Etapa √önica",
         "8": "72",
         "9": "BLOCO 05",
         "10": "463",
         "11": "203",
         "12": "10903",
         "13": "PEDRO BRUNO BARBOSA DA SILVA",
         "14": "28670",
         "15": "ANNA LUIZA PEREIRA DE SOUSA",
         "16": "417",
         "17": "Sim",
         "18": "2025-11-28 17:33:49",
         "63": "2025-12-01 09:17:41.247589",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 19,
         "key": 19
        },
        {
         "0": "6432",
         "1": "2025-12-01 09:14:24",
         "2": "S",
         "3": "6432",
         "4": "2025-11-29 14:22:48",
         "5": "FSA013 - VILA L√ìTUS",
         "6": "33",
         "7": "Etapa √önica",
         "8": "72",
         "9": "BLOCO 18",
         "10": "476",
         "11": "02",
         "12": "11102",
         "13": "REGIANNE CRISTINA DE ALMEIDA GONCALVES",
         "14": "29681",
         "15": "EMMANUELY FERNANDA DO NASCIMENTO",
         "16": "449",
         "17": "Sim",
         "18": "2025-11-29 14:22:48",
         "63": "2025-12-01 09:17:41.247589",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 20,
         "key": 20
        },
        {
         "0": "6213",
         "1": "2025-12-01 09:52:36",
         "2": "S",
         "3": "6213",
         "4": "2025-10-09 15:18:17",
         "5": "AGL30 - VILA GIRASSOL",
         "6": "27",
         "7": "Etapa Unica",
         "8": "62",
         "9": "BLOCO L",
         "10": "401",
         "11": "3",
         "12": "9612",
         "13": "PRISCILLA CAMPOS DA SILVA",
         "14": "26945",
         "15": "Micael dos Santos Pires - CLT",
         "16": "338",
         "17": "Sim",
         "18": "2025-10-09 15:18:17",
         "63": "2025-12-01 10:02:54.279211",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 21,
         "key": 21
        },
        {
         "0": "6271",
         "1": "2025-12-01 09:35:57",
         "2": "S",
         "3": "6271",
         "4": "2025-10-20 19:06:52",
         "5": "FSA013 - VILA L√ìTUS",
         "6": "33",
         "7": "Etapa √önica",
         "8": "72",
         "9": "BLOCO 17",
         "10": "475",
         "11": "303",
         "12": "11099",
         "13": "GEOVANNA RENATA COSTA ALMEIDA",
         "14": "28530",
         "15": "STEFFANY SILVA PINTO",
         "16": "467",
         "17": "Sim",
         "18": "2025-10-27 16:57:04",
         "23": "Sim",
         "24": "2025-10-30 11:45:02",
         "39": "Sim",
         "40": "2025-10-30 11:14:50",
         "56": "2025-10-30 10:46:13",
         "63": "2025-12-01 10:02:54.279211",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 22,
         "key": 22
        },
        {
         "0": "6398",
         "1": "2025-12-01 09:36:26",
         "2": "S",
         "3": "6398",
         "4": "2025-11-19 16:19:10",
         "5": "FSA013 - VILA L√ìTUS",
         "6": "33",
         "7": "Etapa √önica",
         "8": "72",
         "9": "BLOCO 15",
         "10": "473",
         "11": "202",
         "12": "11062",
         "13": "NAELY GOMES DE MELO",
         "14": "29360",
         "15": "Leonard Luis Mogena Perez - CLT",
         "16": "397",
         "17": "Sim",
         "18": "2025-11-19 16:19:10",
         "63": "2025-12-01 10:02:54.279211",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 23,
         "key": 23
        },
        {
         "0": "6415",
         "1": "2025-12-01 09:19:35",
         "2": "S",
         "3": "6415",
         "4": "2025-11-25 15:35:59",
         "5": "FSA013 - VILA L√ìTUS",
         "6": "33",
         "7": "Etapa √önica",
         "8": "72",
         "9": "BLOCO 08",
         "10": "466",
         "11": "302",
         "12": "10954",
         "13": "JOHNNY VIEIRA DE CASTRO",
         "14": "29500",
         "15": "DANILA PEREIRA BORGES",
         "16": "446",
         "17": "Sim",
         "18": "2025-11-25 15:35:59",
         "63": "2025-12-01 09:29:47.3969",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 24,
         "key": 24
        },
        {
         "0": "6420",
         "1": "2025-12-01 09:18:49",
         "2": "S",
         "3": "6420",
         "4": "2025-11-26 19:32:14",
         "5": "FSA010 - VILA JARDINS",
         "6": "26",
         "7": "Etapa Unica",
         "8": "61",
         "9": "BLOCO C",
         "10": "358",
         "11": "APTO 202",
         "12": "8979",
         "13": "CAMILLY VICT√ìRIA MENDON√áA DE JESUS",
         "14": "29531",
         "15": "DAIANA SOARES DA ROCHA",
         "16": "393",
         "17": "Sim",
         "18": "2025-11-26 19:32:14",
         "63": "2025-12-01 09:29:47.3969",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 25,
         "key": 25
        },
        {
         "0": "6422",
         "1": "2025-12-01 09:18:18",
         "2": "S",
         "3": "6422",
         "4": "2025-11-27 10:48:43",
         "5": "FSA013 - VILA L√ìTUS",
         "6": "33",
         "7": "Etapa √önica",
         "8": "72",
         "9": "BLOCO 07",
         "10": "465",
         "11": "103",
         "12": "10931",
         "13": "ANA FLAVIA TAVARES DA SILVA",
         "14": "29563",
         "15": "Camila Fernandes da Silva",
         "16": "291",
         "17": "Sim",
         "18": "2025-11-27 10:48:43",
         "63": "2025-12-01 09:29:47.3969",
         "64": "2025-12-01",
         "65": "CVCRM",
         "index": 26,
         "key": 26
        }
       ],
       "schema": [
        {
         "key": "0",
         "name": "referencia",
         "type": "int"
        },
        {
         "key": "1",
         "name": "referencia_data",
         "type": "timestamp"
        },
        {
         "key": "2",
         "name": "ativo",
         "type": "string"
        },
        {
         "key": "3",
         "name": "idreserva",
         "type": "string"
        },
        {
         "key": "4",
         "name": "data_criacao_reserva",
         "type": "timestamp"
        },
        {
         "key": "5",
         "name": "empreendimento",
         "type": "string"
        },
        {
         "key": "6",
         "name": "idempreendimento",
         "type": "string"
        },
        {
         "key": "7",
         "name": "etapa",
         "type": "string"
        },
        {
         "key": "8",
         "name": "idetapa",
         "type": "string"
        },
        {
         "key": "9",
         "name": "bloco",
         "type": "string"
        },
        {
         "key": "10",
         "name": "idbloco",
         "type": "string"
        },
        {
         "key": "11",
         "name": "unidade",
         "type": "string"
        },
        {
         "key": "12",
         "name": "idunidade",
         "type": "string"
        },
        {
         "key": "13",
         "name": "cliente",
         "type": "string"
        },
        {
         "key": "14",
         "name": "idcliente",
         "type": "string"
        },
        {
         "key": "15",
         "name": "corretor",
         "type": "string"
        },
        {
         "key": "16",
         "name": "idcorretor",
         "type": "string"
        },
        {
         "key": "17",
         "name": "flag_inicio",
         "type": "string"
        },
        {
         "key": "18",
         "name": "data_flag_inicio",
         "type": "timestamp"
        },
        {
         "key": "19",
         "name": "flag_inicio_semproposta",
         "type": "string"
        },
        {
         "key": "20",
         "name": "data_flag_inicio_semproposta",
         "type": "timestamp"
        },
        {
         "key": "21",
         "name": "flag_vencida",
         "type": "string"
        },
        {
         "key": "22",
         "name": "data_flag_vencida",
         "type": "timestamp"
        },
        {
         "key": "23",
         "name": "flag_vendida",
         "type": "string"
        },
        {
         "key": "24",
         "name": "data_flag_vendida",
         "type": "timestamp"
        },
        {
         "key": "25",
         "name": "flag_distrato",
         "type": "string"
        },
        {
         "key": "26",
         "name": "data_flag_distrato",
         "type": "timestamp"
        },
        {
         "key": "27",
         "name": "flag_analisada",
         "type": "string"
        },
        {
         "key": "28",
         "name": "data_flag_analisada",
         "type": "timestamp"
        },
        {
         "key": "29",
         "name": "flag_aprovada",
         "type": "string"
        },
        {
         "key": "30",
         "name": "data_flag_aprovada",
         "type": "timestamp"
        },
        {
         "key": "31",
         "name": "flag_aprovada_comercial",
         "type": "string"
        },
        {
         "key": "32",
         "name": "data_flag_aprovada_comercial",
         "type": "timestamp"
        },
        {
         "key": "33",
         "name": "flag_vender_sienge",
         "type": "string"
        },
        {
         "key": "34",
         "name": "data_flag_vender_sienge",
         "type": "timestamp"
        },
        {
         "key": "35",
         "name": "flag_cancelada_sienge",
         "type": "string"
        },
        {
         "key": "36",
         "name": "data_flag_cancelada_sienge",
         "type": "timestamp"
        },
        {
         "key": "37",
         "name": "flag_cancelada",
         "type": "string"
        },
        {
         "key": "38",
         "name": "data_flag_cancelada",
         "type": "timestamp"
        },
        {
         "key": "39",
         "name": "flag_pode_faturar",
         "type": "string"
        },
        {
         "key": "40",
         "name": "data_flag_pode_faturar",
         "type": "timestamp"
        },
        {
         "key": "41",
         "name": "flag_documentos",
         "type": "string"
        },
        {
         "key": "42",
         "name": "data_flag_documentos",
         "type": "timestamp"
        },
        {
         "key": "43",
         "name": "flag_contrato",
         "type": "string"
        },
        {
         "key": "44",
         "name": "data_flag_contrato",
         "type": "timestamp"
        },
        {
         "key": "45",
         "name": "flag_aguardando_faturamento",
         "type": "string"
        },
        {
         "key": "46",
         "name": "data_flag_aguardando_faturamento",
         "type": "timestamp"
        },
        {
         "key": "47",
         "name": "flag_faturada",
         "type": "string"
        },
        {
         "key": "48",
         "name": "data_flag_faturada",
         "type": "timestamp"
        },
        {
         "key": "49",
         "name": "flag_pendente_assinatura_eletronica",
         "type": "timestamp"
        },
        {
         "key": "50",
         "name": "data_flag_pendente_assinatura_eletronica",
         "type": "timestamp"
        },
        {
         "key": "51",
         "name": "flag_pendente_assinatura_eletronica_segundo_envelope",
         "type": "timestamp"
        },
        {
         "key": "52",
         "name": "data_flag_pendente_assinatura_eletronica_segundo_envelope",
         "type": "timestamp"
        },
        {
         "key": "53",
         "name": "flag_assinatura_clientes_associados",
         "type": "timestamp"
        },
        {
         "key": "54",
         "name": "data_flag_assinatura_clientes_associados",
         "type": "timestamp"
        },
        {
         "key": "55",
         "name": "flag_assinatura_todas_partes",
         "type": "timestamp"
        },
        {
         "key": "56",
         "name": "data_flag_assinatura_todas_partes",
         "type": "timestamp"
        },
        {
         "key": "57",
         "name": "flag_reprovada_comercial",
         "type": "string"
        },
        {
         "key": "58",
         "name": "data_flag_reprovada_comercial",
         "type": "timestamp"
        },
        {
         "key": "59",
         "name": "flag_pendente_comercial",
         "type": "string"
        },
        {
         "key": "60",
         "name": "data_flag_pendente_comercial",
         "type": "timestamp"
        },
        {
         "key": "61",
         "name": "flag_devolucao_erp",
         "type": "string"
        },
        {
         "key": "62",
         "name": "data_flag_devolucao_erp",
         "type": "timestamp"
        },
        {
         "key": "63",
         "name": "ingest_ts",
         "type": "timestamp"
        },
        {
         "key": "64",
         "name": "ingest_date",
         "type": "date"
        },
        {
         "key": "65",
         "name": "sistema_origem",
         "type": "string"
        }
       ],
       "truncated": false
      },
      "wranglerEntryContext": {
       "candidateVariableNames": [
        "df_reservas_registros_flags_bronze"
       ],
       "dataframeType": "pyspark"
      }
     },
     "type": "Synapse.DataFrame"
    }
   },
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
